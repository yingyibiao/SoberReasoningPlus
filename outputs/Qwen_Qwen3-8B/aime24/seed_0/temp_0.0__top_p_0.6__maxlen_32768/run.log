---- 2025-08-22T08:51:55+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 08:52:07.550600094 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 08:53:19 [__init__.py:244] Automatically detected platform cuda.
Using seed: 0
[I822 08:54:23.127288369 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 08:55:07 [__init__.py:244] Automatically detected platform cuda.
[I822 08:55:19.834579930 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:55:19.838004141 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:55:19.838007968 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:55:19.838016179 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:55:19.838021550 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:55:19.838024340 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:55:19.838028122 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:55:19.838703375 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 08:56:22 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 08:56:22 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 08:56:22 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 08:56:22 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 08:56:22 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 08:56:22 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 08:56:22 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 08:56:22 [__init__.py:244] Automatically detected platform cuda.
[I822 08:56:44.312804304 TCPStore.cpp:274] [c10d - debug] The server has started on port = 52083.
[I822 08:56:44.312823882 TCPStoreLibUvBackend.cpp:1178] [c10d - debug] Uv main loop running
[I822 08:56:44.312930120 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 52083).
[I822 08:56:44.317074777 socket.cpp:946] [c10d] The client socket has connected to [localhost]:52083 on SocketImpl(fd=124, addr=[localhost]:33696, remote=[localhost]:52083).
[I822 08:56:44.323839558 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:52083
[I822 08:56:44.555942216 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 52083).
[I822 08:56:44.560306649 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 52083).
[I822 08:56:44.560403247 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 52083).
[I822 08:56:44.561560712 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 52083).
[I822 08:56:44.562266673 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 52083).
[I822 08:56:44.563722610 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 52083).
[I822 08:56:44.559693961 socket.cpp:946] [c10d] The client socket has connected to [localhost]:52083 on SocketImpl(fd=116, addr=[localhost]:33712, remote=[localhost]:52083).
[I822 08:56:44.566067865 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:52083
[I822 08:56:44.565068439 socket.cpp:946] [c10d] The client socket has connected to [localhost]:52083 on SocketImpl(fd=116, addr=[localhost]:33746, remote=[localhost]:52083).
[I822 08:56:44.567495617 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:52083
[I822 08:56:44.565344574 socket.cpp:946] [c10d] The client socket has connected to [localhost]:52083 on SocketImpl(fd=116, addr=[localhost]:33760, remote=[localhost]:52083).
[I822 08:56:44.567775386 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:52083
[I822 08:56:44.563719500 socket.cpp:946] [c10d] The client socket has connected to [localhost]:52083 on SocketImpl(fd=116, addr=[localhost]:33718, remote=[localhost]:52083).
[I822 08:56:44.568244109 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:52083
[I822 08:56:44.564084577 socket.cpp:946] [c10d] The client socket has connected to [localhost]:52083 on SocketImpl(fd=116, addr=[localhost]:33722, remote=[localhost]:52083).
[I822 08:56:44.568373096 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:52083
[I822 08:56:44.564904540 socket.cpp:946] [c10d] The client socket has connected to [localhost]:52083 on SocketImpl(fd=116, addr=[localhost]:33736, remote=[localhost]:52083).
[I822 08:56:44.569438437 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:52083
[I822 08:56:44.610624511 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL initialization options: size: 8, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 08:56:44.610629621 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL initialization options: size: 8, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 08:56:44.610631106 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 7] ProcessGroupNCCL initialization options: size: 8, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 08:56:44.610632350 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL initialization options: size: 8, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 08:56:44.610633908 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL initialization options: size: 8, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 08:56:44.610639865 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 08:56:44.610640531 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 08:56:44.610641914 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 7] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 08:56:44.610636984 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL initialization options: size: 8, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 08:56:44.610645135 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 08:56:44.610645273 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 08:56:44.610647813 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 08:56:44.615498354 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL initialization options: size: 8, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank6]:[I822 08:56:44.615501211 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 6] ProcessGroupNCCL initialization options: size: 8, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank3]:[I822 08:56:44.615501795 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL initialization options: size: 8, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank2]:[I822 08:56:44.615503229 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL initialization options: size: 8, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank6]:[I822 08:56:44.615508456 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 08:56:44.615508644 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 08:56:44.615508908 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 08:56:44.615509672 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 08:56:44.615509276 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 7] ProcessGroupNCCL initialization options: size: 8, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank7]:[I822 08:56:44.615515873 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 7] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 08:56:44.615513828 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 5] ProcessGroupNCCL initialization options: size: 8, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank5]:[I822 08:56:44.615521158 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 08:56:44.634374075 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 52083).
[I822 08:56:44.635824007 socket.cpp:946] [c10d] The client socket has connected to [localhost]:52083 on SocketImpl(fd=116, addr=[localhost]:33770, remote=[localhost]:52083).
[I822 08:56:44.638580160 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:52083
[I822 08:56:44.639046952 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL initialization options: size: 8, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 08:56:44.639058007 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 08:56:44.639411282 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 4] ProcessGroupNCCL initialization options: size: 8, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank4]:[I822 08:56:44.639419153 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 08:56:44.644988871 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL initialization options: size: 8, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 08:56:44.645002593 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 08:56:44.645305637 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL initialization options: size: 8, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank0]:[I822 08:56:44.645313724 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 08:56:44.769652874 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL initialization options: size: 8, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank3]:[I822 08:56:44.769655318 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL initialization options: size: 8, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank3]:[I822 08:56:44.769662919 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 08:56:44.769662842 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 08:56:44.769662104 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL initialization options: size: 8, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank5]:[I822 08:56:44.769669590 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 08:56:44.769691667 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL initialization options: size: 8, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank0]:[I822 08:56:44.769698865 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 08:56:44.769706634 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL initialization options: size: 8, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank4]:[I822 08:56:44.769705761 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL initialization options: size: 8, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank4]:[I822 08:56:44.769714817 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 08:56:44.769713380 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL initialization options: size: 8, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank6]:[I822 08:56:44.769714542 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL initialization options: size: 8, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank1]:[I822 08:56:44.769721094 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 08:56:44.769722903 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 08:56:44.769722907 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
NCCL version 2.26.2+cuda12.2
libfabric:115:1755853005::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:112:1755853005::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:115:1755853005::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:112:1755853005::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:118:1755853005::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:118:1755853005::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:114:1755853005::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:114:1755853005::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:113:1755853005::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:113:1755853005::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:116:1755853005::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:116:1755853005::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:117:1755853005::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:119:1755853005::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:117:1755853005::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:119:1755853005::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:118:1755853005::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:113:1755853005::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:119:1755853005::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:112:1755853005::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:117:1755853005::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:115:1755853005::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:116:1755853005::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:114:1755853005::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:118:1755853005::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:119:1755853005::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:113:1755853005::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:112:1755853005::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:116:1755853005::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:117:1755853005::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:115:1755853005::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:114:1755853005::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
[rank0]:[I822 08:58:16.761277184 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 5
[rank0]:[I822 08:58:16.761313744 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 08:58:16.761315311 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 7
[rank1]:[I822 08:58:16.761339699 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 08:58:16.761332986 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 11
[rank2]:[I822 08:58:16.761339360 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 9
[rank3]:[I822 08:58:16.761358968 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 08:58:16.761362961 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 08:58:16.761375344 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 13 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 13
[rank4]:[I822 08:58:16.761401297 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 13 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 08:58:16.761390291 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 15 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 15
[rank5]:[I822 08:58:16.761416994 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 15 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 08:58:16.761420782 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 17 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 17
[rank7]:[I822 08:58:16.761426184 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 19 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 19
[rank6]:[I822 08:58:16.761446872 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 17 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 08:58:16.761451865 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 19 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 08:58:16.762779395 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 35 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 35
[rank6]:[I822 08:58:16.762787251 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 33 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 33
[rank7]:[I822 08:58:16.762790246 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 35 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 08:58:16.762794837 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 33 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 08:58:16.762795191 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 29 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 29
[rank0]:[I822 08:58:16.762799649 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 21 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 21
[rank4]:[I822 08:58:16.762802517 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 29 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 08:58:16.762807042 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 21 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 08:58:16.762804875 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 25 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 25
[rank2]:[I822 08:58:16.762812013 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 25 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 08:58:16.762816419 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 23 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 23
[rank1]:[I822 08:58:16.762823662 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 23 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 08:58:16.762862697 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 27 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 27
[rank3]:[I822 08:58:16.762872836 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 27 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 08:58:16.762901172 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 31 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 31
[rank5]:[I822 08:58:16.762912608 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 31 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 08:58:16.763828446 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 7] ProcessGroupNCCL initialization options: size: 8, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank7]:[I822 08:58:16.763838310 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 7] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 08:58:16.763868472 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 6] ProcessGroupNCCL initialization options: size: 8, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank6]:[I822 08:58:16.763878028 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 08:58:16.763913699 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 4] ProcessGroupNCCL initialization options: size: 8, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank4]:[I822 08:58:16.763923366 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 08:58:16.763946679 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 2] ProcessGroupNCCL initialization options: size: 8, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank2]:[I822 08:58:16.763956192 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 08:58:16.763966616 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 5] ProcessGroupNCCL initialization options: size: 8, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank1]:[I822 08:58:16.763969123 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 1] ProcessGroupNCCL initialization options: size: 8, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank5]:[I822 08:58:16.763973975 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 08:58:16.763972846 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 3] ProcessGroupNCCL initialization options: size: 8, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank1]:[I822 08:58:16.763975961 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 08:58:16.763979586 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 08:58:16.764025920 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 0] ProcessGroupNCCL initialization options: size: 8, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank0]:[I822 08:58:16.764038048 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[1;36m(VllmWorker rank=7 pid=119)[0;0m [1;36m(VllmWorker rank=2 pid=114)[0;0m [1;36m(VllmWorker rank=6 pid=118)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=0 pid=112)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=1 pid=113)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=4 pid=116)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=3 pid=115)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:11<00:44, 11.09s/it]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:25<00:39, 13.03s/it]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:27<00:16,  8.11s/it]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:40<00:09, 10.00s/it]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:52<00:00, 10.82s/it]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:52<00:00, 10.58s/it]
[1;36m(VllmWorker rank=0 pid=112)[0;0m 
[rank1]:[I822 09:01:02.482922791 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 1] Using non-blocking mode: 0
[rank0]:[I822 09:01:02.482930242 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 0] Using non-blocking mode: 0
[rank2]:[I822 09:01:02.482932668 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 2] Using non-blocking mode: 0
[rank5]:[I822 09:01:02.482936963 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 5] Using non-blocking mode: 0
[rank4]:[I822 09:01:02.482948001 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 4] Using non-blocking mode: 0
[rank6]:[I822 09:01:02.482954186 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 6] Using non-blocking mode: 0
[rank7]:[I822 09:01:02.482955114 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 7] Using non-blocking mode: 0
[rank3]:[I822 09:01:02.482948645 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 3] Using non-blocking mode: 0
[rank0]:[I822 09:01:02.483172647 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL broadcast unique ID through store took 0.026867 ms
[rank0]:[I822 09:01:02.483203501 NCCLUtils.cpp:75] Rank 0: creating NCCL communicator with mode: blocking
[rank1]:[I822 09:01:02.483316168 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL broadcast unique ID through store took 0.349935 ms
[rank1]:[I822 09:01:02.483338374 NCCLUtils.cpp:75] Rank 1: creating NCCL communicator with mode: blocking
[rank5]:[I822 09:01:02.483329212 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL broadcast unique ID through store took 0.358084 ms
[rank5]:[I822 09:01:02.483351027 NCCLUtils.cpp:75] Rank 5: creating NCCL communicator with mode: blocking
[rank7]:[I822 09:01:02.483341337 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL broadcast unique ID through store took 0.355306 ms
[rank7]:[I822 09:01:02.483362500 NCCLUtils.cpp:75] Rank 7: creating NCCL communicator with mode: blocking
[rank6]:[I822 09:01:02.483377850 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL broadcast unique ID through store took 0.385875 ms
[rank6]:[I822 09:01:02.483399051 NCCLUtils.cpp:75] Rank 6: creating NCCL communicator with mode: blocking
[rank2]:[I822 09:01:02.483374990 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL broadcast unique ID through store took 0.397732 ms
[rank3]:[I822 09:01:02.483402759 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL broadcast unique ID through store took 0.406815 ms
[rank2]:[I822 09:01:02.483424054 NCCLUtils.cpp:75] Rank 2: creating NCCL communicator with mode: blocking
[rank3]:[I822 09:01:02.483424826 NCCLUtils.cpp:75] Rank 3: creating NCCL communicator with mode: blocking
[rank4]:[I822 09:01:02.483404411 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL broadcast unique ID through store took 0.413367 ms
[rank4]:[I822 09:01:02.483452941 NCCLUtils.cpp:75] Rank 4: creating NCCL communicator with mode: blocking
[rank3]:[I822 09:01:03.352695318 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 3] NCCL_DEBUG: WARN
[rank6]:[I822 09:01:03.352804491 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 6] NCCL_DEBUG: WARN
[rank7]:[I822 09:01:03.353008335 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 7] NCCL_DEBUG: WARN
[rank2]:[I822 09:01:03.353121997 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 2] NCCL_DEBUG: WARN
[rank4]:[I822 09:01:03.353200197 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 4] NCCL_DEBUG: WARN
[rank5]:[I822 09:01:03.353374844 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 5] NCCL_DEBUG: WARN
[rank0]:[I822 09:01:03.353501478 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 0] NCCL_DEBUG: WARN
[rank1]:[I822 09:01:03.353966983 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 1] NCCL_DEBUG: WARN
[1;36m(VllmWorker rank=0 pid=112)[0;0m Capturing CUDA graph shapes:   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graph shapes:   1%|         | 1/67 [00:00<00:39,  1.65it/s]Capturing CUDA graph shapes:   3%|         | 2/67 [00:01<00:37,  1.71it/s]Capturing CUDA graph shapes:   4%|         | 3/67 [00:01<00:36,  1.74it/s]Capturing CUDA graph shapes:   6%|         | 4/67 [00:02<00:35,  1.75it/s]Capturing CUDA graph shapes:   7%|         | 5/67 [00:02<00:35,  1.76it/s]Capturing CUDA graph shapes:   9%|         | 6/67 [00:03<00:34,  1.76it/s]Capturing CUDA graph shapes:  10%|         | 7/67 [00:04<00:36,  1.62it/s]Capturing CUDA graph shapes:  12%|        | 8/67 [00:04<00:37,  1.55it/s]Capturing CUDA graph shapes:  13%|        | 9/67 [00:05<00:38,  1.51it/s]Capturing CUDA graph shapes:  15%|        | 10/67 [00:06<00:38,  1.48it/s]Capturing CUDA graph shapes:  16%|        | 11/67 [00:06<00:38,  1.46it/s]Capturing CUDA graph shapes:  18%|        | 12/67 [00:07<00:38,  1.45it/s]Capturing CUDA graph shapes:  19%|        | 13/67 [00:08<00:37,  1.44it/s]Capturing CUDA graph shapes:  21%|        | 14/67 [00:09<00:36,  1.44it/s]Capturing CUDA graph shapes:  22%|       | 15/67 [00:09<00:36,  1.43it/s]Capturing CUDA graph shapes:  24%|       | 16/67 [00:10<00:35,  1.43it/s]Capturing CUDA graph shapes:  25%|       | 17/67 [00:11<00:34,  1.43it/s]Capturing CUDA graph shapes:  27%|       | 18/67 [00:11<00:33,  1.47it/s]Capturing CUDA graph shapes:  28%|       | 19/67 [00:12<00:30,  1.55it/s]Capturing CUDA graph shapes:  30%|       | 20/67 [00:12<00:29,  1.61it/s]Capturing CUDA graph shapes:  31%|      | 21/67 [00:13<00:27,  1.65it/s]Capturing CUDA graph shapes:  33%|      | 22/67 [00:14<00:28,  1.56it/s]Capturing CUDA graph shapes:  34%|      | 23/67 [00:14<00:29,  1.52it/s]Capturing CUDA graph shapes:  36%|      | 24/67 [00:15<00:28,  1.49it/s]Capturing CUDA graph shapes:  37%|      | 25/67 [00:16<00:28,  1.47it/s]Capturing CUDA graph shapes:  39%|      | 26/67 [00:17<00:28,  1.45it/s]Capturing CUDA graph shapes:  40%|      | 27/67 [00:17<00:27,  1.44it/s]Capturing CUDA graph shapes:  42%|     | 28/67 [00:18<00:27,  1.43it/s]Capturing CUDA graph shapes:  43%|     | 29/67 [00:19<00:26,  1.43it/s]Capturing CUDA graph shapes:  45%|     | 30/67 [00:19<00:26,  1.42it/s]Capturing CUDA graph shapes:  46%|     | 31/67 [00:20<00:25,  1.42it/s]Capturing CUDA graph shapes:  48%|     | 32/67 [00:21<00:24,  1.42it/s]Capturing CUDA graph shapes:  49%|     | 33/67 [00:21<00:23,  1.44it/s]Capturing CUDA graph shapes:  51%|     | 34/67 [00:22<00:22,  1.47it/s]Capturing CUDA graph shapes:  52%|    | 35/67 [00:23<00:21,  1.50it/s]Capturing CUDA graph shapes:  54%|    | 36/67 [00:23<00:21,  1.48it/s]Capturing CUDA graph shapes:  55%|    | 37/67 [00:24<00:20,  1.45it/s]Capturing CUDA graph shapes:  57%|    | 38/67 [00:25<00:20,  1.44it/s]Capturing CUDA graph shapes:  58%|    | 39/67 [00:26<00:19,  1.43it/s]Capturing CUDA graph shapes:  60%|    | 40/67 [00:26<00:18,  1.42it/s]Capturing CUDA graph shapes:  61%|    | 41/67 [00:27<00:18,  1.42it/s]Capturing CUDA graph shapes:  63%|   | 42/67 [00:28<00:17,  1.41it/s]Capturing CUDA graph shapes:  64%|   | 43/67 [00:28<00:17,  1.41it/s]Capturing CUDA graph shapes:  66%|   | 44/67 [00:29<00:15,  1.46it/s]Capturing CUDA graph shapes:  67%|   | 45/67 [00:30<00:14,  1.54it/s]Capturing CUDA graph shapes:  69%|   | 46/67 [00:30<00:13,  1.61it/s]Capturing CUDA graph shapes:  70%|   | 47/67 [00:31<00:12,  1.65it/s]Capturing CUDA graph shapes:  72%|  | 48/67 [00:31<00:11,  1.68it/s]Capturing CUDA graph shapes:  73%|  | 49/67 [00:32<00:10,  1.70it/s]Capturing CUDA graph shapes:  75%|  | 50/67 [00:32<00:09,  1.72it/s]Capturing CUDA graph shapes:  76%|  | 51/67 [00:33<00:09,  1.72it/s]Capturing CUDA graph shapes:  78%|  | 52/67 [00:34<00:08,  1.73it/s]Capturing CUDA graph shapes:  79%|  | 53/67 [00:34<00:08,  1.61it/s]Capturing CUDA graph shapes:  81%|  | 54/67 [00:35<00:08,  1.54it/s]Capturing CUDA graph shapes:  82%| | 55/67 [00:36<00:08,  1.49it/s]Capturing CUDA graph shapes:  84%| | 56/67 [00:36<00:07,  1.47it/s]Capturing CUDA graph shapes:  85%| | 57/67 [00:37<00:06,  1.46it/s]Capturing CUDA graph shapes:  87%| | 58/67 [00:38<00:06,  1.44it/s]Capturing CUDA graph shapes:  88%| | 59/67 [00:39<00:05,  1.44it/s]Capturing CUDA graph shapes:  90%| | 60/67 [00:39<00:04,  1.45it/s]Capturing CUDA graph shapes:  91%| | 61/67 [00:40<00:04,  1.45it/s]Capturing CUDA graph shapes:  93%|| 62/67 [00:41<00:03,  1.45it/s]Capturing CUDA graph shapes:  94%|| 63/67 [00:41<00:02,  1.46it/s]Capturing CUDA graph shapes:  96%|| 64/67 [00:42<00:01,  1.50it/s]Capturing CUDA graph shapes:  97%|| 65/67 [00:43<00:01,  1.58it/s]Capturing CUDA graph shapes:  99%|| 66/67 [00:43<00:00,  1.63it/s]Capturing CUDA graph shapes: 100%|| 67/67 [00:44<00:00,  1.68it/s]Capturing CUDA graph shapes: 100%|| 67/67 [00:44<00:00,  1.52it/s]
If you want to use extended_tasks, make sure you installed their dependencies using `pip install -e .[extended_tasks]`.
Careful, the task custom|aime24 is using evaluation data to build the few shot examples.
You cannot select the number of dataset splits for a generative evaluation at the moment. Automatically inferring.
Splits:   0%|          | 0/1 [00:00<?, ?it/s]
Adding requests:   0%|          | 0/30 [00:00<?, ?it/s][AAdding requests: 100%|| 30/30 [00:00<00:00, 2549.26it/s]

Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   3%|         | 1/30 [00:35<17:20, 35.89s/it, est. speed input: 6.21 toks/s, output: 114.41 toks/s][A
Processed prompts:   7%|         | 2/30 [00:36<07:07, 15.27s/it, est. speed input: 11.90 toks/s, output: 225.94 toks/s][A
Processed prompts:  13%|        | 4/30 [00:43<03:20,  7.71s/it, est. speed input: 16.80 toks/s, output: 402.48 toks/s][A
Processed prompts:  17%|        | 5/30 [00:48<02:56,  7.06s/it, est. speed input: 17.82 toks/s, output: 468.01 toks/s][A
Processed prompts:  20%|        | 6/30 [00:51<02:15,  5.65s/it, est. speed input: 21.51 toks/s, output: 556.77 toks/s][A
Processed prompts:  23%|       | 7/30 [00:56<02:08,  5.59s/it, est. speed input: 23.01 toks/s, output: 614.07 toks/s][A
Processed prompts:  27%|       | 8/30 [01:01<01:54,  5.21s/it, est. speed input: 24.50 toks/s, output: 680.74 toks/s][A
Processed prompts:  30%|       | 9/30 [01:10<02:19,  6.64s/it, est. speed input: 23.08 toks/s, output: 694.99 toks/s][A
Processed prompts:  33%|      | 10/30 [01:32<03:40, 11.03s/it, est. speed input: 19.81 toks/s, output: 642.59 toks/s][A
Processed prompts:  37%|      | 11/30 [01:34<02:37,  8.28s/it, est. speed input: 22.46 toks/s, output: 736.88 toks/s][A
Processed prompts:  40%|      | 12/30 [01:36<01:58,  6.57s/it, est. speed input: 23.41 toks/s, output: 824.27 toks/s][A
Processed prompts:  43%|     | 13/30 [01:46<02:07,  7.50s/it, est. speed input: 23.54 toks/s, output: 855.88 toks/s][A
Processed prompts:  47%|     | 14/30 [01:50<01:43,  6.49s/it, est. speed input: 23.92 toks/s, output: 930.28 toks/s][A
Processed prompts:  50%|     | 15/30 [01:58<01:45,  7.04s/it, est. speed input: 23.64 toks/s, output: 971.01 toks/s][A
Processed prompts:  53%|    | 16/30 [02:02<01:24,  6.05s/it, est. speed input: 24.52 toks/s, output: 1047.02 toks/s][A
Processed prompts:  57%|    | 17/30 [02:04<01:01,  4.70s/it, est. speed input: 25.44 toks/s, output: 1139.74 toks/s][A
Processed prompts:  60%|    | 18/30 [02:04<00:41,  3.47s/it, est. speed input: 26.56 toks/s, output: 1240.12 toks/s][A
Processed prompts:  63%|   | 19/30 [02:05<00:28,  2.63s/it, est. speed input: 27.81 toks/s, output: 1339.07 toks/s][A
Processed prompts:  67%|   | 20/30 [02:18<00:56,  5.66s/it, est. speed input: 26.90 toks/s, output: 1321.68 toks/s][A
Processed prompts:  70%|   | 21/30 [02:25<00:56,  6.31s/it, est. speed input: 26.67 toks/s, output: 1357.02 toks/s][A
Processed prompts:  73%|  | 22/30 [02:48<01:28, 11.09s/it, est. speed input: 24.74 toks/s, output: 1283.92 toks/s][A
Processed prompts:  77%|  | 23/30 [02:56<01:12, 10.37s/it, est. speed input: 24.58 toks/s, output: 1327.77 toks/s][A
Processed prompts:  80%|  | 24/30 [03:26<01:37, 16.29s/it, est. speed input: 23.33 toks/s, output: 1242.95 toks/s][A
Processed prompts:  83%| | 25/30 [03:27<00:58, 11.72s/it, est. speed input: 24.09 toks/s, output: 1345.19 toks/s][A
Processed prompts:  87%| | 26/30 [04:03<01:15, 18.92s/it, est. speed input: 21.25 toks/s, output: 1258.59 toks/s][A
Processed prompts:  90%| | 27/30 [04:19<00:53, 17.99s/it, est. speed input: 20.61 toks/s, output: 1293.37 toks/s][A
Processed prompts:  93%|| 28/30 [04:48<00:42, 21.40s/it, est. speed input: 19.33 toks/s, output: 1275.34 toks/s][A
Processed prompts: 100%|| 30/30 [04:48<00:00, 21.40s/it, est. speed input: 20.47 toks/s, output: 1502.21 toks/s][AProcessed prompts: 100%|| 30/30 [04:48<00:00,  9.63s/it, est. speed input: 20.47 toks/s, output: 1502.21 toks/s]
Splits: 100%|| 1/1 [04:49<00:00, 289.06s/it]Splits: 100%|| 1/1 [04:49<00:00, 289.06s/it]
[rank4]:[I822 09:06:41.605377111 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 4] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:06:41.605390322 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 1] Starting to destroy process group, flushing operations.
[rank0]:[I822 09:06:41.605460629 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 0] Starting to destroy process group, flushing operations.
[rank2]:[I822 09:06:41.605476635 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 2] Starting to destroy process group, flushing operations.
[rank7]:[I822 09:06:41.605499595 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 7] Starting to destroy process group, flushing operations.
[rank6]:[I822 09:06:41.606079275 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 6] Starting to destroy process group, flushing operations.
[rank5]:[I822 09:06:41.606093540 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 5] Starting to destroy process group, flushing operations.
[rank3]:[I822 09:06:41.606301072 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 3] Starting to destroy process group, flushing operations.
[rank4]:[I822 09:06:41.607774071 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 4] Operations flushed, joining watchdog thread.
[rank1]:[I822 09:06:41.607778165 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 1] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:06:41.607834380 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 0] Operations flushed, joining watchdog thread.
[rank7]:[I822 09:06:41.607840337 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 7] Operations flushed, joining watchdog thread.
[rank4]:[I822 09:06:41.607859173 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 4] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:06:41.607887187 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 09:06:41.607919189 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 7] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:06:41.607921457 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:06:41.607960665 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 2] Operations flushed, joining watchdog thread.
[rank2]:[I822 09:06:41.608069555 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 09:06:41.608470001 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 5] Operations flushed, joining watchdog thread.
[rank6]:[I822 09:06:41.608481211 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 6] Operations flushed, joining watchdog thread.
[rank5]:[I822 09:06:41.608554299 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 5] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 09:06:41.608577324 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 6] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 09:06:41.608613919 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 3] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:06:41.608709942 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:06:41.051540322 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 0] Destroy complete.
[rank2]:[I822 09:06:41.065359117 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 2] Destroy complete.
[rank7]:[I822 09:06:42.068243198 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 7] Destroy complete.
[rank3]:[I822 09:06:42.068242046 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 3] Destroy complete.
[rank1]:[I822 09:06:42.077250759 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 1] Destroy complete.
[rank4]:[I822 09:06:42.080870696 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 4] Destroy complete.
[rank6]:[I822 09:06:42.081097078 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 6] Destroy complete.
[rank5]:[I822 09:06:42.082014915 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 5] Destroy complete.
[rank0]:[I822 09:06:42.121624589 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 09:06:42.121651807 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 09:06:42.122602604 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 5 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 09:06:42.122610048 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 5 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:06:42.122659804 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 5 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:06:42.122665988 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 5 Rank 0] Destroy complete.
[rank0]:[I822 09:06:42.122697954 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 09:06:42.122716060 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 09:06:42.131659435 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL destructor entered.
[rank1]:[I822 09:06:42.131692708 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 09:06:42.131744779 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 7 Rank 0] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:06:42.131749891 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 7 Rank 0] Operations flushed, joining watchdog thread.
[rank1]:[I822 09:06:42.131796641 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 7 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:06:42.131802382 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 7 Rank 0] Destroy complete.
[rank1]:[I822 09:06:42.131822359 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL destructor entered.
[rank1]:[I822 09:06:42.131835405 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 09:06:42.140668536 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 21 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 09:06:42.140678040 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 21 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:06:42.140727666 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 21 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:06:42.140733388 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 21 Rank 0] Destroy complete.
[rank0]:[I822 09:06:42.140750759 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 21 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 09:06:42.140772570 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 21 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 09:06:42.143723228 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 23 Rank 0] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:06:42.143730798 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 23 Rank 0] Operations flushed, joining watchdog thread.
[rank1]:[I822 09:06:42.143790135 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 23 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:06:42.143794898 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 23 Rank 0] Destroy complete.
[rank1]:[I822 09:06:42.143807560 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 23 Rank 0] ProcessGroupNCCL destructor entered.
[rank1]:[I822 09:06:42.143835992 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 23 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 09:06:42.144481744 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:06:42.144503664 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 09:06:42.144550140 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 9 Rank 0] Starting to destroy process group, flushing operations.
[rank2]:[I822 09:06:42.144555382 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 9 Rank 0] Operations flushed, joining watchdog thread.
[rank2]:[I822 09:06:42.144605184 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 9 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:06:42.144610847 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 9 Rank 0] Destroy complete.
[rank2]:[I822 09:06:42.144631499 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:06:42.144650102 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:06:42.154528276 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL destructor entered.
[rank5]:[I822 09:06:42.154530656 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL destructor entered.
[rank4]:[I822 09:06:42.154538046 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL destructor entered.
[rank5]:[I822 09:06:42.154550634 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:06:42.154551100 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 09:06:42.154561908 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL heart beat monitor thread joined.
[rank5]:[I822 09:06:42.154595388 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 15 Rank 0] Starting to destroy process group, flushing operations.
[rank3]:[I822 09:06:42.154597515 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 11 Rank 0] Starting to destroy process group, flushing operations.
[rank3]:[I822 09:06:42.154600896 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 11 Rank 0] Operations flushed, joining watchdog thread.
[rank5]:[I822 09:06:42.154601922 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 15 Rank 0] Operations flushed, joining watchdog thread.
[rank4]:[I822 09:06:42.154614840 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 13 Rank 0] Starting to destroy process group, flushing operations.
[rank4]:[I822 09:06:42.154618972 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 13 Rank 0] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:06:42.154660336 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 11 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 09:06:42.154667040 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 11 Rank 0] Destroy complete.
[rank5]:[I822 09:06:42.154672098 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 15 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 09:06:42.154677184 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 15 Rank 0] Destroy complete.
[rank3]:[I822 09:06:42.154688998 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL destructor entered.
[rank4]:[I822 09:06:42.154689104 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 13 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 09:06:42.154693587 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 13 Rank 0] Destroy complete.
[rank5]:[I822 09:06:42.154697361 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 15 Rank 0] ProcessGroupNCCL destructor entered.
[rank3]:[I822 09:06:42.154711043 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 09:06:42.154716441 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 13 Rank 0] ProcessGroupNCCL destructor entered.
[rank5]:[I822 09:06:42.154717814 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 15 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 09:06:42.154732428 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 13 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 09:06:42.157498728 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 1] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:06:42.157509272 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 1] Operations flushed, joining watchdog thread.
[rank1]:[I822 09:06:42.157574042 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:06:42.157580407 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 1] Destroy complete.
[rank0]:[I822 09:06:42.158996272 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 09:06:42.159004335 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:06:42.159058459 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:06:42.159063082 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 0] Destroy complete.
[rank2]:[I822 09:06:42.161038400 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 25 Rank 0] Starting to destroy process group, flushing operations.
[rank2]:[I822 09:06:42.161046373 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 25 Rank 0] Operations flushed, joining watchdog thread.
[rank2]:[I822 09:06:42.161109651 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 25 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:06:42.161114235 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 25 Rank 0] Destroy complete.
[rank2]:[I822 09:06:42.161127297 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 25 Rank 0] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:06:42.161148547 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 25 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 09:06:42.164536561 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL destructor entered.
[rank7]:[I822 09:06:42.164565873 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 09:06:42.164609176 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 19 Rank 0] Starting to destroy process group, flushing operations.
[rank7]:[I822 09:06:42.164614622 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 19 Rank 0] Operations flushed, joining watchdog thread.
[rank7]:[I822 09:06:42.164681908 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 19 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 09:06:42.164687827 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 19 Rank 0] Destroy complete.
[rank7]:[I822 09:06:42.164711242 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 19 Rank 0] ProcessGroupNCCL destructor entered.
[rank7]:[I822 09:06:42.164730491 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 19 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 09:06:42.164749225 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL destructor entered.
[rank6]:[I822 09:06:42.164776109 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 09:06:42.164821005 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 17 Rank 0] Starting to destroy process group, flushing operations.
[rank6]:[I822 09:06:42.164826213 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 17 Rank 0] Operations flushed, joining watchdog thread.
[rank6]:[I822 09:06:42.164874915 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 17 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 09:06:42.164881282 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 17 Rank 0] Destroy complete.
[rank6]:[I822 09:06:42.164900242 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 17 Rank 0] ProcessGroupNCCL destructor entered.
[rank6]:[I822 09:06:42.164915284 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 17 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 09:06:42.168079073 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 29 Rank 0] Starting to destroy process group, flushing operations.
[rank4]:[I822 09:06:42.168091061 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 29 Rank 0] Operations flushed, joining watchdog thread.
[rank4]:[I822 09:06:42.168154594 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 29 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 09:06:42.168159178 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 29 Rank 0] Destroy complete.
[rank4]:[I822 09:06:42.168174181 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 29 Rank 0] ProcessGroupNCCL destructor entered.
[rank4]:[I822 09:06:42.168193228 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 29 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:06:42.170516557 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 27 Rank 0] Starting to destroy process group, flushing operations.
[rank3]:[I822 09:06:42.170527622 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 27 Rank 0] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:06:42.170584707 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 27 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 09:06:42.170589319 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 27 Rank 0] Destroy complete.
[rank3]:[I822 09:06:42.170603361 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 27 Rank 0] ProcessGroupNCCL destructor entered.
[rank3]:[I822 09:06:42.170622712 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 27 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 09:06:42.172160194 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 2] Starting to destroy process group, flushing operations.
[rank2]:[I822 09:06:42.172169384 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 2] Operations flushed, joining watchdog thread.
[rank2]:[I822 09:06:42.172229294 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:06:42.172233907 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 2] Destroy complete.
[rank5]:[I822 09:06:42.172407019 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 31 Rank 0] Starting to destroy process group, flushing operations.
[rank5]:[I822 09:06:42.172415878 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 31 Rank 0] Operations flushed, joining watchdog thread.
[rank5]:[I822 09:06:42.172471350 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 31 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 09:06:42.172475941 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 31 Rank 0] Destroy complete.
[rank5]:[I822 09:06:42.172489702 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 31 Rank 0] ProcessGroupNCCL destructor entered.
[rank5]:[I822 09:06:42.172509886 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 31 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 09:06:42.178227108 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 35 Rank 0] Starting to destroy process group, flushing operations.
[rank7]:[I822 09:06:42.178239880 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 35 Rank 0] Operations flushed, joining watchdog thread.
[rank7]:[I822 09:06:42.178301939 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 35 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 09:06:42.178306453 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 35 Rank 0] Destroy complete.
[rank7]:[I822 09:06:42.178321491 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 35 Rank 0] ProcessGroupNCCL destructor entered.
[rank7]:[I822 09:06:42.178338596 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 35 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 09:06:42.184430340 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 33 Rank 0] Starting to destroy process group, flushing operations.
[rank6]:[I822 09:06:42.184440955 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 33 Rank 0] Operations flushed, joining watchdog thread.
[rank6]:[I822 09:06:42.184482168 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 33 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 09:06:42.184486403 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 33 Rank 0] Destroy complete.
[rank6]:[I822 09:06:42.184499101 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 33 Rank 0] ProcessGroupNCCL destructor entered.
[rank6]:[I822 09:06:42.184527847 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 33 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank5]:[I822 09:06:42.184612935 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 5] Starting to destroy process group, flushing operations.
[rank5]:[I822 09:06:42.184620325 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 5] Operations flushed, joining watchdog thread.
[rank5]:[I822 09:06:42.184665920 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 5] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 09:06:42.184670183 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 5] Destroy complete.
[rank3]:[I822 09:06:42.186102461 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 3] Starting to destroy process group, flushing operations.
[rank3]:[I822 09:06:42.186125985 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 3] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:06:42.186175193 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 09:06:42.186179621 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 3] Destroy complete.
[rank4]:[I822 09:06:42.188077974 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 4] Starting to destroy process group, flushing operations.
[rank4]:[I822 09:06:42.188086782 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 4] Operations flushed, joining watchdog thread.
[rank4]:[I822 09:06:42.188129268 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 4] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 09:06:42.188133384 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 4] Destroy complete.
[rank7]:[I822 09:06:42.194835349 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 7] Starting to destroy process group, flushing operations.
[rank7]:[I822 09:06:42.194843732 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 7] Operations flushed, joining watchdog thread.
[rank7]:[I822 09:06:42.194898784 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 7] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 09:06:42.194903271 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 7] Destroy complete.
[rank6]:[I822 09:06:42.197869520 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 6] Starting to destroy process group, flushing operations.
[rank6]:[I822 09:06:42.197879658 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 6] Operations flushed, joining watchdog thread.
[rank6]:[I822 09:06:42.197923060 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 6] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 09:06:42.197927154 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 6] Destroy complete.
[rank0]:[I822 09:06:42.230471242 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 09:06:42.230492522 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 09:06:42.230515389 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 09:06:42.230520045 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:06:42.230564576 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:06:42.230570085 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 0] Destroy complete.
[rank0]:[I822 09:06:42.230586461 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 09:06:42.230602471 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 09:06:42.234280418 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 1] ProcessGroupNCCL destructor entered.
[rank1]:[I822 09:06:42.234299867 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 09:06:42.234320156 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 1] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:06:42.234324776 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 1] Operations flushed, joining watchdog thread.
[rank1]:[I822 09:06:42.234376367 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:06:42.234382360 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 1] Destroy complete.
[rank1]:[I822 09:06:42.234398640 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL destructor entered.
[rank1]:[I822 09:06:42.234417952 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 09:06:42.245390396 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 2] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:06:42.245413752 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 09:06:42.245436887 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 2] Starting to destroy process group, flushing operations.
[rank2]:[I822 09:06:42.245441695 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 2] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:06:42.245438471 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 3] ProcessGroupNCCL destructor entered.
[rank3]:[I822 09:06:42.245456227 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:06:42.245480211 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 3] Starting to destroy process group, flushing operations.
[rank3]:[I822 09:06:42.245489871 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 3] Operations flushed, joining watchdog thread.
[rank2]:[I822 09:06:42.245491161 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:06:42.245496043 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 2] Destroy complete.
[rank2]:[I822 09:06:42.245512529 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:06:42.245525147 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 09:06:42.245528485 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 4] ProcessGroupNCCL destructor entered.
[rank4]:[I822 09:06:42.245546690 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 4] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:06:42.245550447 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 09:06:42.245556160 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 3] Destroy complete.
[rank4]:[I822 09:06:42.245569669 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 4] Starting to destroy process group, flushing operations.
[rank4]:[I822 09:06:42.245574676 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 4] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:06:42.245580931 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL destructor entered.
[rank3]:[I822 09:06:42.245602460 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 09:06:42.245628690 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 4] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 09:06:42.245634560 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 4] Destroy complete.
[rank4]:[I822 09:06:42.245651551 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 4] ProcessGroupNCCL destructor entered.
[rank4]:[I822 09:06:42.245670939 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 4] ProcessGroupNCCL heart beat monitor thread joined.
[rank5]:[I822 09:06:42.255564121 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 5] ProcessGroupNCCL destructor entered.
[rank5]:[I822 09:06:42.255587361 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 5] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 09:06:42.255601141 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 6] ProcessGroupNCCL destructor entered.
[rank5]:[I822 09:06:42.255605823 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 5] Starting to destroy process group, flushing operations.
[rank5]:[I822 09:06:42.255609192 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 5] Operations flushed, joining watchdog thread.
[rank6]:[I822 09:06:42.255620541 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 6] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 09:06:42.255642438 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 6] Starting to destroy process group, flushing operations.
[rank6]:[I822 09:06:42.255648145 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 6] Operations flushed, joining watchdog thread.
[rank5]:[I822 09:06:42.255658089 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 5] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 09:06:42.255656078 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 7] ProcessGroupNCCL destructor entered.
[rank5]:[I822 09:06:42.255661779 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 5] Destroy complete.
[rank7]:[I822 09:06:42.255674637 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 7] ProcessGroupNCCL heart beat monitor thread joined.
[rank5]:[I822 09:06:42.255677005 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 5] ProcessGroupNCCL destructor entered.
[rank6]:[I822 09:06:42.255696865 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 6] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 09:06:42.255701037 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 6] Destroy complete.
[rank7]:[I822 09:06:42.255700601 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 7] Starting to destroy process group, flushing operations.
[rank7]:[I822 09:06:42.255703963 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 7] Operations flushed, joining watchdog thread.
[rank5]:[I822 09:06:42.255707048 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 5] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 09:06:42.255716033 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 6] ProcessGroupNCCL destructor entered.
[rank6]:[I822 09:06:42.255727335 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 6] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 09:06:42.255763194 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 7] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 09:06:42.255771413 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 7] Destroy complete.
[rank7]:[I822 09:06:42.255791853 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 7] ProcessGroupNCCL destructor entered.
[rank7]:[I822 09:06:42.255811173 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 7] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 09:06:42.277833105 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 09:06:42.277846253 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:06:42.277890145 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:06:42.277892821 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 0] Destroy complete.
[I822 09:06:42.277934083 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL destructor entered.
[I822 09:06:42.277951817 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[I822 09:06:42.277995246 TCPStoreLibUvBackend.cpp:130] [c10d - debug] Read callback failed. code:-4095 name:EOF desc:end of file
[I822 09:06:42.278066471 TCPStoreLibUvBackend.cpp:1105] [c10d - debug] Store exit requested

[I822 09:06:42.278072989 TCPStoreLibUvBackend.cpp:1181] [c10d - debug] UV main loop done: res:1
[I822 09:06:42.278076066 TCPStoreLibUvBackend.cpp:1187] [c10d - debug] Walking live handles prior to closing clients
[I822 09:06:42.278079228 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:06:42.278082057 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:06:42.278084205 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:06:42.278086289 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:06:42.278088285 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:06:42.278090559 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:06:42.278092529 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:06:42.278094482 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:06:42.278155843 TCPStoreLibUvBackend.cpp:1197] [c10d - debug] Walking live handles after closing clients
[I822 09:06:42.278160578 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:06:42.278162745 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:06:42.278164699 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:06:42.278166611 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:06:42.278168663 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:06:42.278170735 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:06:42.278172828 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:06:42.278174720 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:06:42.278177261 TCPStoreLibUvBackend.cpp:1206] [c10d] uv_loop_close failed with:-16 errn:EBUSY desc:resource busy or locked
[I822 09:06:42.278203760 TCPStoreLibUvBackend.cpp:1216] [c10d] uv_loop cleanup finished.
[rank2]:[I822 09:06:42.297949100 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 2] Starting to destroy process group, flushing operations.
[rank2]:[I822 09:06:42.297960142 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 2] Operations flushed, joining watchdog thread.
[rank1]:[I822 09:06:42.297967143 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 1] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:06:42.297974671 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 1] Operations flushed, joining watchdog thread.
[rank2]:[I822 09:06:42.298005852 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:06:42.298010302 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 2] Destroy complete.
[rank1]:[I822 09:06:42.298020863 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:06:42.298023606 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 1] Destroy complete.
[I822 09:06:42.298047712 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL destructor entered.
[I822 09:06:42.298058288 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL destructor entered.
[I822 09:06:42.298064425 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[I822 09:06:42.298075736 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:06:42.312298541 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 3] Starting to destroy process group, flushing operations.
[rank3]:[I822 09:06:42.312313131 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 3] Operations flushed, joining watchdog thread.
[rank4]:[I822 09:06:42.312332635 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 4] Starting to destroy process group, flushing operations.
[rank4]:[I822 09:06:42.312340718 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 4] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:06:42.312362933 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 09:06:42.312367529 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 3] Destroy complete.
[rank4]:[I822 09:06:42.312402581 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 4] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 09:06:42.312409934 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 4] Destroy complete.
[I822 09:06:42.312412565 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL destructor entered.
[rank5]:[I822 09:06:42.312414041 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 5] Starting to destroy process group, flushing operations.
[rank5]:[I822 09:06:42.312422163 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 5] Operations flushed, joining watchdog thread.
[I822 09:06:42.312430985 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 09:06:42.312437315 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 6] Starting to destroy process group, flushing operations.
[rank6]:[I822 09:06:42.312446372 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 6] Operations flushed, joining watchdog thread.
[I822 09:06:42.312450749 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL destructor entered.
[I822 09:06:42.312471717 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL heart beat monitor thread joined.
[rank5]:[I822 09:06:42.312473437 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 5] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 09:06:42.312476371 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 5] Destroy complete.
[rank6]:[I822 09:06:42.312492664 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 6] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 09:06:42.312492357 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 7] Starting to destroy process group, flushing operations.
[rank6]:[I822 09:06:42.312499620 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 6] Destroy complete.
[rank7]:[I822 09:06:42.312500456 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 7] Operations flushed, joining watchdog thread.
[I822 09:06:42.312510821 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL destructor entered.
[I822 09:06:42.312529580 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL heart beat monitor thread joined.
[I822 09:06:42.312538051 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL destructor entered.
[rank7]:[I822 09:06:42.312559691 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 7] Watchdog joined, destroying NCCL communicators.
[I822 09:06:42.312559565 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 09:06:42.312568683 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 7] Destroy complete.
[I822 09:06:42.312609909 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 7] ProcessGroupNCCL destructor entered.
[I822 09:06:42.312634903 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 7] ProcessGroupNCCL heart beat monitor thread joined.
|     Task      |Version|     Metric     |Value|   |Stderr|
|---------------|------:|----------------|----:|---|-----:|
|all            |       |extractive_match|  0.7|  |0.0851|
|custom:aime24:0|      1|extractive_match|  0.7|  |0.0851|

Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|| 1/1 [00:00<00:00, 27.18ba/s]
+ set +x
---- 2025-08-22T09:06:49+00:00 RUN END ----
---- 2025-08-22T23:27:08+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
Traceback (most recent call last):
  File "/code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py", line 21, in <module>
    from lm_eval import evaluator
ModuleNotFoundError: No module named 'lm_eval'
---- 2025-08-22T23:27:33+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
Traceback (most recent call last):
  File "/code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py", line 21, in <module>
    from lm_eval import evaluator
ModuleNotFoundError: No module named 'lm_eval'
---- 2025-08-22T23:28:05+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
Traceback (most recent call last):
  File "/code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py", line 21, in <module>
    from lm_eval import evaluator
ModuleNotFoundError: No module named 'lm_eval'
---- 2025-08-22T23:29:06+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:29:09.118396957 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:29:56 [__init__.py:244] Automatically detected platform cuda.
Using seed: 0
[I822 23:30:55.237610281 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:31:40 [__init__.py:244] Automatically detected platform cuda.
[I822 23:31:52.883806457 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 23:31:52.883814191 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 23:31:52.883821043 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 23:31:52.883804629 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 23:31:52.883826259 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 23:31:52.883829657 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 23:31:52.883830615 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 23:31:52.884518424 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:32:55 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 23:32:55 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 23:32:55 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 23:32:56 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 23:32:56 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 23:32:56 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 23:32:56 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 23:32:56 [__init__.py:244] Automatically detected platform cuda.
[I822 23:33:17.701047398 TCPStore.cpp:274] [c10d - debug] The server has started on port = 37461.
[I822 23:33:17.701067255 TCPStoreLibUvBackend.cpp:1178] [c10d - debug] Uv main loop running
[I822 23:33:17.701172431 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 37461).
[I822 23:33:17.702822582 socket.cpp:946] [c10d] The client socket has connected to [localhost]:37461 on SocketImpl(fd=124, addr=[localhost]:53854, remote=[localhost]:37461).
[I822 23:33:17.708226646 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:37461
[I822 23:33:17.972076744 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 37461).
[I822 23:33:17.973853212 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 37461).
[I822 23:33:17.976186710 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 37461).
[I822 23:33:17.976303863 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 37461).
[I822 23:33:17.976642243 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 37461).
[I822 23:33:17.977228828 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 37461).
[I822 23:33:17.975737467 socket.cpp:946] [c10d] The client socket has connected to [localhost]:37461 on SocketImpl(fd=116, addr=[localhost]:53884, remote=[localhost]:37461).
[I822 23:33:17.978275817 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:37461
[I822 23:33:17.978563719 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 37461).
[I822 23:33:17.975375720 socket.cpp:946] [c10d] The client socket has connected to [localhost]:37461 on SocketImpl(fd=116, addr=[localhost]:53870, remote=[localhost]:37461).
[I822 23:33:17.979974724 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:37461
[I822 23:33:17.977718564 socket.cpp:946] [c10d] The client socket has connected to [localhost]:37461 on SocketImpl(fd=116, addr=[localhost]:53900, remote=[localhost]:37461).
[I822 23:33:17.980209628 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:37461
[I822 23:33:17.977909614 socket.cpp:946] [c10d] The client socket has connected to [localhost]:37461 on SocketImpl(fd=116, addr=[localhost]:53916, remote=[localhost]:37461).
[I822 23:33:17.980311700 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:37461
[I822 23:33:17.978163940 socket.cpp:946] [c10d] The client socket has connected to [localhost]:37461 on SocketImpl(fd=116, addr=[localhost]:53920, remote=[localhost]:37461).
[I822 23:33:17.980508761 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:37461
[I822 23:33:17.979978494 socket.cpp:946] [c10d] The client socket has connected to [localhost]:37461 on SocketImpl(fd=116, addr=[localhost]:53938, remote=[localhost]:37461).
[I822 23:33:17.982454057 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:37461
[I822 23:33:17.978704678 socket.cpp:946] [c10d] The client socket has connected to [localhost]:37461 on SocketImpl(fd=116, addr=[localhost]:53932, remote=[localhost]:37461).
[I822 23:33:17.982867797 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:37461
[I822 23:33:17.007755738 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL initialization options: size: 8, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 23:33:17.007761781 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL initialization options: size: 8, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 23:33:17.007763047 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL initialization options: size: 8, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 23:33:17.007765551 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 7] ProcessGroupNCCL initialization options: size: 8, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 23:33:17.007771301 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 23:33:17.007773162 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 23:33:17.007774629 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 23:33:17.007776425 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 7] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 23:33:17.007774965 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL initialization options: size: 8, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 23:33:17.007780469 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL initialization options: size: 8, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 23:33:17.007791329 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 23:33:17.007791035 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 23:33:17.008103385 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL initialization options: size: 8, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank2]:[I822 23:33:17.008108328 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL initialization options: size: 8, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank3]:[I822 23:33:17.008111879 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 23:33:17.008114759 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 23:33:17.008118422 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 6] ProcessGroupNCCL initialization options: size: 8, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank4]:[I822 23:33:17.008121739 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 4] ProcessGroupNCCL initialization options: size: 8, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank6]:[I822 23:33:17.008125120 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 23:33:17.008125349 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 7] ProcessGroupNCCL initialization options: size: 8, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank4]:[I822 23:33:17.008128243 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 23:33:17.008131681 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 7] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 23:33:17.008147933 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL initialization options: size: 8, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank1]:[I822 23:33:17.008156964 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 23:33:17.008566448 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL initialization options: size: 8, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 23:33:17.008581503 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 23:33:17.008883946 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 5] ProcessGroupNCCL initialization options: size: 8, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank5]:[I822 23:33:17.008892330 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 23:33:17.017872731 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL initialization options: size: 8, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 23:33:17.017887447 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 23:33:17.018202652 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL initialization options: size: 8, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank0]:[I822 23:33:17.018213489 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 23:33:18.110639128 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL initialization options: size: 8, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank1]:[I822 23:33:18.110642214 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL initialization options: size: 8, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank1]:[I822 23:33:18.110648746 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 23:33:18.110649278 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 23:33:18.110686912 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL initialization options: size: 8, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank7]:[I822 23:33:18.110693556 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 23:33:18.110735797 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL initialization options: size: 8, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank2]:[I822 23:33:18.110745125 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 23:33:18.110756213 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL initialization options: size: 8, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank6]:[I822 23:33:18.110763042 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 23:33:18.110827132 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL initialization options: size: 8, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank5]:[I822 23:33:18.110837613 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 23:33:18.110863755 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL initialization options: size: 8, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank4]:[I822 23:33:18.110870350 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 23:33:18.110878465 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL initialization options: size: 8, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank3]:[I822 23:33:18.110885436 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
NCCL version 2.26.2+cuda12.2
libfabric:117:1755905598::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:117:1755905598::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:118:1755905598::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:118:1755905598::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:114:1755905598::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:114:1755905598::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:113:1755905598::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:113:1755905598::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:119:1755905598::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:116:1755905598::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:119:1755905598::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:116:1755905598::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:115:1755905598::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:112:1755905598::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:115:1755905598::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:112:1755905598::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:117:1755905598::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:117:1755905598::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:118:1755905598::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:116:1755905598::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:119:1755905598::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:113:1755905598::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:114:1755905598::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:112:1755905598::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:115:1755905598::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:118:1755905598::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:116:1755905598::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:119:1755905598::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:113:1755905598::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:114:1755905598::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:115:1755905598::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:112:1755905598::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
[rank0]:[I822 23:34:50.743898401 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 5
[rank0]:[I822 23:34:50.743930294 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 23:34:50.743973465 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 7
[rank2]:[I822 23:34:50.743976570 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 9
[rank2]:[I822 23:34:50.744016434 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 23:34:50.744014532 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 23:34:50.744015770 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 11
[rank3]:[I822 23:34:50.744038044 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 23:34:50.744032445 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 13 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 13
[rank4]:[I822 23:34:50.744057648 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 13 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 23:34:50.744103959 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 15 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 15
[rank7]:[I822 23:34:50.744108870 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 19 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 19
[rank5]:[I822 23:34:50.744125419 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 15 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 23:34:50.744130921 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 19 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 23:34:50.744130870 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 17 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 17
[rank6]:[I822 23:34:50.744152315 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 17 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 23:34:50.745315659 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 35 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 35
[rank7]:[I822 23:34:50.745326032 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 35 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 23:34:50.745328234 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 27 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 27
[rank3]:[I822 23:34:50.745335511 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 27 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 23:34:50.745334631 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 21 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 21
[rank6]:[I822 23:34:50.745337987 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 33 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 33
[rank0]:[I822 23:34:50.745342161 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 21 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 23:34:50.745345166 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 33 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 23:34:50.745358121 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 29 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 29
[rank4]:[I822 23:34:50.745369214 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 29 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 23:34:50.745474246 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 25 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 25
[rank2]:[I822 23:34:50.745485176 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 25 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 23:34:50.745498960 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 23 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 23
[rank1]:[I822 23:34:50.745512305 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 23 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 23:34:50.745519646 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 31 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 31
[rank5]:[I822 23:34:50.745527750 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 31 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 23:34:50.746276896 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 7] ProcessGroupNCCL initialization options: size: 8, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank7]:[I822 23:34:50.746286406 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 7] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 23:34:50.746319694 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 6] ProcessGroupNCCL initialization options: size: 8, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank6]:[I822 23:34:50.746329054 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 23:34:50.746354978 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 3] ProcessGroupNCCL initialization options: size: 8, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank3]:[I822 23:34:50.746363982 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 23:34:50.746362318 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 5] ProcessGroupNCCL initialization options: size: 8, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank4]:[I822 23:34:50.746365940 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 4] ProcessGroupNCCL initialization options: size: 8, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank5]:[I822 23:34:50.746369603 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 23:34:50.746372526 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 23:34:50.746416760 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 2] ProcessGroupNCCL initialization options: size: 8, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank0]:[I822 23:34:50.746420411 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 0] ProcessGroupNCCL initialization options: size: 8, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank0]:[I822 23:34:50.746426988 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 23:34:50.746427247 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 23:34:50.746428994 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 1] ProcessGroupNCCL initialization options: size: 8, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank1]:[I822 23:34:50.746435900 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=6 pid=118)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=7 pid=119)[0;0m [1;36m(VllmWorker rank=1 pid=113)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=2 pid=114)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=0 pid=112)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=4 pid=116)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:01,  2.20it/s]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:00<00:01,  2.16it/s]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:01<00:00,  3.23it/s]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:01<00:00,  2.61it/s]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:01<00:00,  2.56it/s]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:01<00:00,  2.57it/s]
[1;36m(VllmWorker rank=0 pid=112)[0;0m 
[rank4]:[I822 23:36:36.228163248 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 4] Using non-blocking mode: 0
[rank6]:[I822 23:36:36.228173994 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 6] Using non-blocking mode: 0
[rank7]:[I822 23:36:36.228183171 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 7] Using non-blocking mode: 0
[rank2]:[I822 23:36:36.228196590 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 2] Using non-blocking mode: 0
[rank3]:[I822 23:36:36.228198584 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 3] Using non-blocking mode: 0
[rank1]:[I822 23:36:36.228263877 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 1] Using non-blocking mode: 0
[rank5]:[I822 23:36:36.228394917 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 5] Using non-blocking mode: 0
[rank0]:[I822 23:36:36.228453547 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 0] Using non-blocking mode: 0
[rank0]:[I822 23:36:36.228699954 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL broadcast unique ID through store took 0.035319 ms
[rank0]:[I822 23:36:36.228731824 NCCLUtils.cpp:75] Rank 0: creating NCCL communicator with mode: blocking
[rank6]:[I822 23:36:36.228927619 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL broadcast unique ID through store took 0.722219 ms
[rank4]:[I822 23:36:36.228941812 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL broadcast unique ID through store took 0.737029 ms
[rank6]:[I822 23:36:36.228976080 NCCLUtils.cpp:75] Rank 6: creating NCCL communicator with mode: blocking
[rank7]:[I822 23:36:36.228953555 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL broadcast unique ID through store took 0.738522 ms
[rank1]:[I822 23:36:36.228972516 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL broadcast unique ID through store took 0.679821 ms
[rank4]:[I822 23:36:36.228989089 NCCLUtils.cpp:75] Rank 4: creating NCCL communicator with mode: blocking
[rank1]:[I822 23:36:36.228995582 NCCLUtils.cpp:75] Rank 1: creating NCCL communicator with mode: blocking
[rank7]:[I822 23:36:36.228999722 NCCLUtils.cpp:75] Rank 7: creating NCCL communicator with mode: blocking
[rank2]:[I822 23:36:36.228966452 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL broadcast unique ID through store took 0.738789 ms
[rank3]:[I822 23:36:36.228996326 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL broadcast unique ID through store took 0.766347 ms
[rank2]:[I822 23:36:36.229036294 NCCLUtils.cpp:75] Rank 2: creating NCCL communicator with mode: blocking
[rank3]:[I822 23:36:36.229040319 NCCLUtils.cpp:75] Rank 3: creating NCCL communicator with mode: blocking
[rank5]:[I822 23:36:36.229026016 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL broadcast unique ID through store took 0.598168 ms
[rank5]:[I822 23:36:36.229074646 NCCLUtils.cpp:75] Rank 5: creating NCCL communicator with mode: blocking
[rank3]:[I822 23:36:37.076803436 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 3] NCCL_DEBUG: WARN
[rank0]:[I822 23:36:37.076957503 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 0] NCCL_DEBUG: WARN
[rank7]:[I822 23:36:37.077065160 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 7] NCCL_DEBUG: WARN
[rank5]:[I822 23:36:37.077166670 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 5] NCCL_DEBUG: WARN
[rank2]:[I822 23:36:37.077230433 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 2] NCCL_DEBUG: WARN
[rank1]:[I822 23:36:37.077232750 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 1] NCCL_DEBUG: WARN
[rank4]:[I822 23:36:37.077298771 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 4] NCCL_DEBUG: WARN
[rank6]:[I822 23:36:37.077412510 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 6] NCCL_DEBUG: WARN
[1;36m(VllmWorker rank=0 pid=112)[0;0m Capturing CUDA graph shapes:   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graph shapes:   1%|         | 1/67 [00:00<00:34,  1.92it/s]Capturing CUDA graph shapes:   3%|         | 2/67 [00:01<00:34,  1.87it/s]Capturing CUDA graph shapes:   4%|         | 3/67 [00:01<00:37,  1.71it/s]Capturing CUDA graph shapes:   6%|         | 4/67 [00:02<00:39,  1.59it/s]Capturing CUDA graph shapes:   7%|         | 5/67 [00:03<00:40,  1.53it/s]Capturing CUDA graph shapes:   9%|         | 6/67 [00:03<00:40,  1.49it/s]Capturing CUDA graph shapes:  10%|         | 7/67 [00:04<00:40,  1.48it/s]Capturing CUDA graph shapes:  12%|        | 8/67 [00:05<00:40,  1.47it/s]Capturing CUDA graph shapes:  13%|        | 9/67 [00:05<00:39,  1.47it/s]Capturing CUDA graph shapes:  15%|        | 10/67 [00:06<00:38,  1.47it/s]Capturing CUDA graph shapes:  16%|        | 11/67 [00:07<00:38,  1.47it/s]Capturing CUDA graph shapes:  18%|        | 12/67 [00:07<00:37,  1.47it/s]Capturing CUDA graph shapes:  19%|        | 13/67 [00:08<00:36,  1.46it/s]Capturing CUDA graph shapes:  21%|        | 14/67 [00:09<00:36,  1.47it/s]Capturing CUDA graph shapes:  22%|       | 15/67 [00:09<00:35,  1.47it/s]Capturing CUDA graph shapes:  24%|       | 16/67 [00:10<00:34,  1.46it/s]Capturing CUDA graph shapes:  25%|       | 17/67 [00:11<00:33,  1.50it/s]Capturing CUDA graph shapes:  27%|       | 18/67 [00:11<00:31,  1.57it/s]Capturing CUDA graph shapes:  28%|       | 19/67 [00:12<00:29,  1.63it/s]Capturing CUDA graph shapes:  30%|       | 20/67 [00:12<00:28,  1.68it/s]Capturing CUDA graph shapes:  31%|      | 21/67 [00:13<00:26,  1.71it/s]Capturing CUDA graph shapes:  33%|      | 22/67 [00:14<00:28,  1.61it/s]Capturing CUDA graph shapes:  34%|      | 23/67 [00:14<00:28,  1.56it/s]Capturing CUDA graph shapes:  36%|      | 24/67 [00:15<00:28,  1.53it/s]Capturing CUDA graph shapes:  37%|      | 25/67 [00:16<00:27,  1.51it/s]Capturing CUDA graph shapes:  39%|      | 26/67 [00:16<00:27,  1.49it/s]Capturing CUDA graph shapes:  40%|      | 27/67 [00:17<00:27,  1.48it/s]Capturing CUDA graph shapes:  42%|     | 28/67 [00:18<00:26,  1.48it/s]Capturing CUDA graph shapes:  43%|     | 29/67 [00:18<00:25,  1.50it/s]Capturing CUDA graph shapes:  45%|     | 30/67 [00:19<00:23,  1.57it/s]Capturing CUDA graph shapes:  46%|     | 31/67 [00:20<00:22,  1.63it/s]Capturing CUDA graph shapes:  48%|     | 32/67 [00:20<00:21,  1.67it/s]Capturing CUDA graph shapes:  49%|     | 33/67 [00:21<00:20,  1.69it/s]Capturing CUDA graph shapes:  51%|     | 34/67 [00:21<00:19,  1.71it/s]Capturing CUDA graph shapes:  52%|    | 35/67 [00:22<00:18,  1.72it/s]Capturing CUDA graph shapes:  54%|    | 36/67 [00:22<00:17,  1.74it/s]Capturing CUDA graph shapes:  55%|    | 37/67 [00:23<00:17,  1.74it/s]Capturing CUDA graph shapes:  57%|    | 38/67 [00:24<00:17,  1.64it/s]Capturing CUDA graph shapes:  58%|    | 39/67 [00:24<00:17,  1.58it/s]Capturing CUDA graph shapes:  60%|    | 40/67 [00:25<00:17,  1.54it/s]Capturing CUDA graph shapes:  61%|    | 41/67 [00:26<00:17,  1.51it/s]Capturing CUDA graph shapes:  63%|   | 42/67 [00:26<00:16,  1.49it/s]Capturing CUDA graph shapes:  64%|   | 43/67 [00:27<00:16,  1.48it/s]Capturing CUDA graph shapes:  66%|   | 44/67 [00:28<00:15,  1.47it/s]Capturing CUDA graph shapes:  67%|   | 45/67 [00:29<00:15,  1.47it/s]Capturing CUDA graph shapes:  69%|   | 46/67 [00:29<00:14,  1.46it/s]Capturing CUDA graph shapes:  70%|   | 47/67 [00:30<00:13,  1.46it/s]Capturing CUDA graph shapes:  72%|  | 48/67 [00:31<00:13,  1.46it/s]Capturing CUDA graph shapes:  73%|  | 49/67 [00:31<00:12,  1.48it/s]Capturing CUDA graph shapes:  75%|  | 50/67 [00:32<00:11,  1.47it/s]Capturing CUDA graph shapes:  76%|  | 51/67 [00:33<00:10,  1.46it/s]Capturing CUDA graph shapes:  78%|  | 52/67 [00:33<00:10,  1.44it/s]Capturing CUDA graph shapes:  79%|  | 53/67 [00:34<00:09,  1.43it/s]Capturing CUDA graph shapes:  81%|  | 54/67 [00:35<00:09,  1.42it/s]Capturing CUDA graph shapes:  82%| | 55/67 [00:36<00:08,  1.40it/s]Capturing CUDA graph shapes:  84%| | 56/67 [00:36<00:07,  1.40it/s]Capturing CUDA graph shapes:  85%| | 57/67 [00:37<00:07,  1.40it/s]Capturing CUDA graph shapes:  87%| | 58/67 [00:38<00:06,  1.41it/s]Capturing CUDA graph shapes:  88%| | 59/67 [00:38<00:05,  1.41it/s]Capturing CUDA graph shapes:  90%| | 60/67 [00:39<00:04,  1.42it/s]Capturing CUDA graph shapes:  91%| | 61/67 [00:40<00:04,  1.44it/s]Capturing CUDA graph shapes:  93%|| 62/67 [00:40<00:03,  1.45it/s]Capturing CUDA graph shapes:  94%|| 63/67 [00:41<00:02,  1.46it/s]Capturing CUDA graph shapes:  96%|| 64/67 [00:42<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|| 65/67 [00:42<00:01,  1.61it/s]Capturing CUDA graph shapes:  99%|| 66/67 [00:43<00:00,  1.68it/s]Capturing CUDA graph shapes: 100%|| 67/67 [00:43<00:00,  1.73it/s]Capturing CUDA graph shapes: 100%|| 67/67 [00:43<00:00,  1.53it/s]
If you want to use extended_tasks, make sure you installed their dependencies using `pip install -e .[extended_tasks]`.
Careful, the task custom|aime24 is using evaluation data to build the few shot examples.
You cannot select the number of dataset splits for a generative evaluation at the moment. Automatically inferring.
Splits:   0%|          | 0/1 [00:00<?, ?it/s]
Adding requests:   0%|          | 0/30 [00:00<?, ?it/s][AAdding requests: 100%|| 30/30 [00:00<00:00, 8432.46it/s]

Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   3%|         | 1/30 [00:33<16:20, 33.82s/it, est. speed input: 6.59 toks/s, output: 114.68 toks/s][A
Processed prompts:   7%|         | 2/30 [00:38<07:53, 16.92s/it, est. speed input: 11.72 toks/s, output: 213.06 toks/s][A
Processed prompts:  10%|         | 3/30 [00:44<05:11, 11.56s/it, est. speed input: 13.77 toks/s, output: 300.40 toks/s][A
Processed prompts:  13%|        | 4/30 [00:44<03:05,  7.13s/it, est. speed input: 16.79 toks/s, output: 410.34 toks/s][A
Processed prompts:  17%|        | 5/30 [00:45<02:05,  5.02s/it, est. speed input: 19.32 toks/s, output: 510.97 toks/s][A
Processed prompts:  20%|        | 6/30 [00:49<01:50,  4.62s/it, est. speed input: 22.14 toks/s, output: 582.83 toks/s][A
Processed prompts:  23%|       | 7/30 [00:59<02:23,  6.24s/it, est. speed input: 21.97 toks/s, output: 598.99 toks/s][A
Processed prompts:  27%|       | 8/30 [01:01<01:48,  4.93s/it, est. speed input: 23.55 toks/s, output: 688.47 toks/s][A
Processed prompts:  30%|       | 9/30 [01:03<01:23,  3.95s/it, est. speed input: 26.52 toks/s, output: 779.08 toks/s][A
Processed prompts:  33%|      | 10/30 [01:09<01:35,  4.77s/it, est. speed input: 26.04 toks/s, output: 814.99 toks/s][A
Processed prompts:  37%|      | 11/30 [01:12<01:19,  4.21s/it, est. speed input: 27.06 toks/s, output: 891.72 toks/s][A
Processed prompts:  40%|      | 12/30 [01:13<00:57,  3.20s/it, est. speed input: 30.38 toks/s, output: 990.24 toks/s][A
Processed prompts:  43%|     | 13/30 [01:14<00:41,  2.45s/it, est. speed input: 32.62 toks/s, output: 1090.28 toks/s][A
Processed prompts:  47%|     | 14/30 [01:35<02:07,  8.00s/it, est. speed input: 27.22 toks/s, output: 960.14 toks/s] [A
Processed prompts:  50%|     | 15/30 [01:45<02:12,  8.85s/it, est. speed input: 25.91 toks/s, output: 970.46 toks/s][A
Processed prompts:  53%|    | 16/30 [01:52<01:55,  8.23s/it, est. speed input: 26.48 toks/s, output: 1019.88 toks/s][A
Processed prompts:  57%|    | 17/30 [02:00<01:47,  8.27s/it, est. speed input: 26.27 toks/s, output: 1057.43 toks/s][A
Processed prompts:  60%|    | 18/30 [02:16<02:05, 10.42s/it, est. speed input: 24.58 toks/s, output: 1045.07 toks/s][A
Processed prompts:  63%|   | 19/30 [02:53<03:23, 18.53s/it, est. speed input: 20.34 toks/s, output: 925.79 toks/s] [A
Processed prompts:  67%|   | 20/30 [03:02<02:36, 15.69s/it, est. speed input: 20.90 toks/s, output: 985.38 toks/s][A
Processed prompts:  70%|   | 21/30 [03:08<01:53, 12.60s/it, est. speed input: 21.19 toks/s, output: 1062.40 toks/s][A
Processed prompts:  73%|  | 22/30 [03:11<01:18,  9.84s/it, est. speed input: 21.81 toks/s, output: 1148.83 toks/s][A
Processed prompts:  77%|  | 23/30 [03:14<00:55,  7.86s/it, est. speed input: 22.32 toks/s, output: 1235.08 toks/s][A
Processed prompts:  80%|  | 24/30 [03:25<00:51,  8.62s/it, est. speed input: 22.10 toks/s, output: 1278.49 toks/s][A
Processed prompts:  83%| | 25/30 [03:29<00:35,  7.17s/it, est. speed input: 22.46 toks/s, output: 1361.52 toks/s][A
Processed prompts:  87%| | 26/30 [03:31<00:22,  5.63s/it, est. speed input: 23.03 toks/s, output: 1454.79 toks/s][A
Processed prompts:  90%| | 27/30 [04:00<00:38, 12.77s/it, est. speed input: 20.95 toks/s, output: 1385.95 toks/s][A
Processed prompts:  93%|| 28/30 [04:02<00:19,  9.64s/it, est. speed input: 22.73 toks/s, output: 1481.98 toks/s][A
Processed prompts:  97%|| 29/30 [04:27<00:14, 14.19s/it, est. speed input: 21.20 toks/s, output: 1456.25 toks/s][A
Processed prompts: 100%|| 30/30 [04:48<00:00, 16.15s/it, est. speed input: 20.49 toks/s, output: 1465.12 toks/s][A
Processed prompts: 100%|| 30/30 [04:48<00:00, 16.15s/it, est. speed input: 20.49 toks/s, output: 1465.12 toks/s][AProcessed prompts: 100%|| 30/30 [04:48<00:00,  9.62s/it, est. speed input: 20.49 toks/s, output: 1465.12 toks/s]
Splits: 100%|| 1/1 [04:48<00:00, 288.70s/it]Splits: 100%|| 1/1 [04:48<00:00, 288.70s/it]
[rank1]:[I822 23:42:14.193055638 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 1] Starting to destroy process group, flushing operations.
[rank3]:[I822 23:42:14.193062650 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 3] Starting to destroy process group, flushing operations.
[rank0]:[I822 23:42:14.193084772 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 0] Starting to destroy process group, flushing operations.
[rank4]:[I822 23:42:14.193091601 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 4] Starting to destroy process group, flushing operations.
[rank5]:[I822 23:42:14.193092748 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 5] Starting to destroy process group, flushing operations.
[rank6]:[I822 23:42:14.193114307 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 6] Starting to destroy process group, flushing operations.
[rank7]:[I822 23:42:14.193147704 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 7] Starting to destroy process group, flushing operations.
[rank2]:[I822 23:42:14.193972833 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 2] Starting to destroy process group, flushing operations.
[rank3]:[I822 23:42:14.195459283 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 3] Operations flushed, joining watchdog thread.
[rank1]:[I822 23:42:14.195460393 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 1] Operations flushed, joining watchdog thread.
[rank4]:[I822 23:42:14.195473519 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 4] Operations flushed, joining watchdog thread.
[rank0]:[I822 23:42:14.195474419 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 0] Operations flushed, joining watchdog thread.
[rank7]:[I822 23:42:14.195475130 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 7] Operations flushed, joining watchdog thread.
[rank5]:[I822 23:42:14.195499585 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 5] Operations flushed, joining watchdog thread.
[rank6]:[I822 23:42:14.195518261 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 6] Operations flushed, joining watchdog thread.
[rank3]:[I822 23:42:14.195557114 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 23:42:14.195558291 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 7] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 23:42:14.195565076 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 23:42:14.195569337 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 4] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 23:42:14.195590274 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 5] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 23:42:14.195579035 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 23:42:14.195670363 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 6] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 23:42:14.196365798 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 2] Operations flushed, joining watchdog thread.
[rank2]:[I822 23:42:14.196456282 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 23:42:14.258726549 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 0] Destroy complete.
[rank1]:[I822 23:42:14.286691858 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 1] Destroy complete.
[rank7]:[I822 23:42:14.288369199 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 7] Destroy complete.
[rank3]:[I822 23:42:14.293702351 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 3] Destroy complete.
[rank2]:[I822 23:42:14.294480846 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 2] Destroy complete.
[rank6]:[I822 23:42:14.295288296 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 6] Destroy complete.
[rank5]:[I822 23:42:14.296719709 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 5] Destroy complete.
[rank4]:[I822 23:42:14.298150188 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 4] Destroy complete.
[rank0]:[I822 23:42:14.330772737 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 23:42:14.330798782 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 23:42:14.330867752 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 5 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 23:42:14.330872994 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 5 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 23:42:14.330921896 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 5 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 23:42:14.330927296 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 5 Rank 0] Destroy complete.
[rank0]:[I822 23:42:14.330957643 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 23:42:14.330977034 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 23:42:14.340824702 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL destructor entered.
[rank1]:[I822 23:42:14.340846248 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 23:42:14.340890700 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 7 Rank 0] Starting to destroy process group, flushing operations.
[rank1]:[I822 23:42:14.340895870 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 7 Rank 0] Operations flushed, joining watchdog thread.
[rank1]:[I822 23:42:14.340946221 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 7 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 23:42:14.340951857 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 7 Rank 0] Destroy complete.
[rank1]:[I822 23:42:14.340970007 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL destructor entered.
[rank1]:[I822 23:42:14.340989897 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 23:42:14.341275571 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 21 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 23:42:14.341291786 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 21 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 23:42:14.341349018 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 21 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 23:42:14.341355005 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 21 Rank 0] Destroy complete.
[rank0]:[I822 23:42:14.341380710 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 21 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 23:42:14.341396211 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 21 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 23:42:14.351630445 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 23 Rank 0] Starting to destroy process group, flushing operations.
[rank1]:[I822 23:42:14.351646593 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 23 Rank 0] Operations flushed, joining watchdog thread.
[rank1]:[I822 23:42:14.351713842 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 23 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 23:42:14.351719852 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 23 Rank 0] Destroy complete.
[rank1]:[I822 23:42:14.351749044 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 23 Rank 0] ProcessGroupNCCL destructor entered.
[rank1]:[I822 23:42:14.351763653 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 23 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 23:42:14.357935838 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 23:42:14.357943768 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 23:42:14.357998001 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 23:42:14.358002611 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 0] Destroy complete.
[rank2]:[I822 23:42:14.360955884 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL destructor entered.
[rank2]:[I822 23:42:14.360977984 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 23:42:14.361021339 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 9 Rank 0] Starting to destroy process group, flushing operations.
[rank2]:[I822 23:42:14.361026543 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 9 Rank 0] Operations flushed, joining watchdog thread.
[rank2]:[I822 23:42:14.361073413 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 9 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 23:42:14.361080864 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 9 Rank 0] Destroy complete.
[rank2]:[I822 23:42:14.361107638 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL destructor entered.
[rank2]:[I822 23:42:14.361124850 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 23:42:14.369641148 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 1] Starting to destroy process group, flushing operations.
[rank1]:[I822 23:42:14.369651282 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 1] Operations flushed, joining watchdog thread.
[rank1]:[I822 23:42:14.369707321 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 23:42:14.369711881 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 1] Destroy complete.
[rank3]:[I822 23:42:14.371023049 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL destructor entered.
[rank3]:[I822 23:42:14.371046422 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 23:42:14.371047970 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL destructor entered.
[rank4]:[I822 23:42:14.371068473 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 23:42:14.371088430 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 11 Rank 0] Starting to destroy process group, flushing operations.
[rank3]:[I822 23:42:14.371093649 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 11 Rank 0] Operations flushed, joining watchdog thread.
[rank4]:[I822 23:42:14.371115584 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 13 Rank 0] Starting to destroy process group, flushing operations.
[rank4]:[I822 23:42:14.371122410 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 13 Rank 0] Operations flushed, joining watchdog thread.
[rank3]:[I822 23:42:14.371140020 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 11 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 23:42:14.371145582 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 11 Rank 0] Destroy complete.
[rank3]:[I822 23:42:14.371163522 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL destructor entered.
[rank4]:[I822 23:42:14.371172645 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 13 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 23:42:14.371176092 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 13 Rank 0] Destroy complete.
[rank3]:[I822 23:42:14.371184226 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 23:42:14.371196174 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 13 Rank 0] ProcessGroupNCCL destructor entered.
[rank4]:[I822 23:42:14.371219031 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 13 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 23:42:14.374720276 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 25 Rank 0] Starting to destroy process group, flushing operations.
[rank2]:[I822 23:42:14.374728290 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 25 Rank 0] Operations flushed, joining watchdog thread.
[rank2]:[I822 23:42:14.374774333 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 25 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 23:42:14.374778593 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 25 Rank 0] Destroy complete.
[rank2]:[I822 23:42:14.374792052 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 25 Rank 0] ProcessGroupNCCL destructor entered.
[rank2]:[I822 23:42:14.374821717 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 25 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 23:42:14.383095167 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 27 Rank 0] Starting to destroy process group, flushing operations.
[rank3]:[I822 23:42:14.383103917 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 27 Rank 0] Operations flushed, joining watchdog thread.
[rank3]:[I822 23:42:14.383162098 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 27 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 23:42:14.383166446 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 27 Rank 0] Destroy complete.
[rank3]:[I822 23:42:14.383179904 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 27 Rank 0] ProcessGroupNCCL destructor entered.
[rank3]:[I822 23:42:14.383211474 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 27 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 23:42:14.385142293 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 2] Starting to destroy process group, flushing operations.
[rank2]:[I822 23:42:14.385149421 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 2] Operations flushed, joining watchdog thread.
[rank2]:[I822 23:42:14.385189562 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 23:42:14.385193846 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 2] Destroy complete.
[rank5]:[I822 23:42:14.388734818 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL destructor entered.
[rank5]:[I822 23:42:14.388753951 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL heart beat monitor thread joined.
[rank5]:[I822 23:42:14.388795638 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 15 Rank 0] Starting to destroy process group, flushing operations.
[rank5]:[I822 23:42:14.388800645 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 15 Rank 0] Operations flushed, joining watchdog thread.
[rank5]:[I822 23:42:14.388846018 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 15 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 23:42:14.388851899 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 15 Rank 0] Destroy complete.
[rank5]:[I822 23:42:14.388872353 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 15 Rank 0] ProcessGroupNCCL destructor entered.
[rank5]:[I822 23:42:14.388891881 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 15 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 23:42:14.389351181 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 29 Rank 0] Starting to destroy process group, flushing operations.
[rank4]:[I822 23:42:14.389359661 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 29 Rank 0] Operations flushed, joining watchdog thread.
[rank4]:[I822 23:42:14.389406048 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 29 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 23:42:14.389410139 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 29 Rank 0] Destroy complete.
[rank4]:[I822 23:42:14.389424719 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 29 Rank 0] ProcessGroupNCCL destructor entered.
[rank4]:[I822 23:42:14.389452822 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 29 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 23:42:14.391099714 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL destructor entered.
[rank7]:[I822 23:42:14.391127470 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 23:42:14.391179991 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 19 Rank 0] Starting to destroy process group, flushing operations.
[rank7]:[I822 23:42:14.391185580 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 19 Rank 0] Operations flushed, joining watchdog thread.
[rank7]:[I822 23:42:14.391259098 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 19 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 23:42:14.391265037 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 19 Rank 0] Destroy complete.
[rank7]:[I822 23:42:14.391286188 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 19 Rank 0] ProcessGroupNCCL destructor entered.
[rank7]:[I822 23:42:14.391304443 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 19 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 23:42:14.395667024 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL destructor entered.
[rank6]:[I822 23:42:14.395704395 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 23:42:14.395753506 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 17 Rank 0] Starting to destroy process group, flushing operations.
[rank6]:[I822 23:42:14.395759052 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 17 Rank 0] Operations flushed, joining watchdog thread.
[rank6]:[I822 23:42:14.395806490 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 17 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 23:42:14.395811656 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 17 Rank 0] Destroy complete.
[rank6]:[I822 23:42:14.395833564 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 17 Rank 0] ProcessGroupNCCL destructor entered.
[rank6]:[I822 23:42:14.395849146 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 17 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 23:42:14.402356989 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 3] Starting to destroy process group, flushing operations.
[rank3]:[I822 23:42:14.402366482 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 3] Operations flushed, joining watchdog thread.
[rank3]:[I822 23:42:14.402408347 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 23:42:14.402412310 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 3] Destroy complete.
[rank4]:[I822 23:42:14.403532358 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 4] Starting to destroy process group, flushing operations.
[rank4]:[I822 23:42:14.403539428 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 4] Operations flushed, joining watchdog thread.
[rank4]:[I822 23:42:14.403588677 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 4] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 23:42:14.403593154 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 4] Destroy complete.
[rank5]:[I822 23:42:14.405075334 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 31 Rank 0] Starting to destroy process group, flushing operations.
[rank5]:[I822 23:42:14.405083294 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 31 Rank 0] Operations flushed, joining watchdog thread.
[rank5]:[I822 23:42:14.405150474 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 31 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 23:42:14.405156052 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 31 Rank 0] Destroy complete.
[rank5]:[I822 23:42:14.405170549 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 31 Rank 0] ProcessGroupNCCL destructor entered.
[rank5]:[I822 23:42:14.405198757 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 31 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 23:42:14.410479266 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 35 Rank 0] Starting to destroy process group, flushing operations.
[rank7]:[I822 23:42:14.410490383 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 35 Rank 0] Operations flushed, joining watchdog thread.
[rank7]:[I822 23:42:14.410543042 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 35 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 23:42:14.410547426 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 35 Rank 0] Destroy complete.
[rank7]:[I822 23:42:14.410562688 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 35 Rank 0] ProcessGroupNCCL destructor entered.
[rank7]:[I822 23:42:14.410580564 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 35 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 23:42:14.411561479 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 33 Rank 0] Starting to destroy process group, flushing operations.
[rank6]:[I822 23:42:14.411572157 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 33 Rank 0] Operations flushed, joining watchdog thread.
[rank6]:[I822 23:42:14.411615471 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 33 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 23:42:14.411619812 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 33 Rank 0] Destroy complete.
[rank6]:[I822 23:42:14.411631959 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 33 Rank 0] ProcessGroupNCCL destructor entered.
[rank6]:[I822 23:42:14.411660242 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 33 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank5]:[I822 23:42:14.422096229 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 5] Starting to destroy process group, flushing operations.
[rank5]:[I822 23:42:14.422112637 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 5] Operations flushed, joining watchdog thread.
[rank5]:[I822 23:42:14.422168038 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 5] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 23:42:14.422172765 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 5] Destroy complete.
[rank6]:[I822 23:42:14.430700334 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 6] Starting to destroy process group, flushing operations.
[rank6]:[I822 23:42:14.430708443 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 6] Operations flushed, joining watchdog thread.
[rank6]:[I822 23:42:14.430769709 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 6] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 23:42:14.430775332 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 6] Destroy complete.
[rank7]:[I822 23:42:14.430846114 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 7] Starting to destroy process group, flushing operations.
[rank7]:[I822 23:42:14.430856230 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 7] Operations flushed, joining watchdog thread.
[rank7]:[I822 23:42:14.430899414 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 7] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 23:42:14.430905484 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 7] Destroy complete.
[rank0]:[I822 23:42:14.438892050 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 23:42:14.438913920 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 23:42:14.438935278 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 23:42:14.438940333 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 23:42:14.438992583 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 23:42:14.438998256 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 0] Destroy complete.
[rank0]:[I822 23:42:14.439017032 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 23:42:14.439035119 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 23:42:14.449012396 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 1] ProcessGroupNCCL destructor entered.
[rank1]:[I822 23:42:14.449036988 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 23:42:14.449063661 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 1] Starting to destroy process group, flushing operations.
[rank1]:[I822 23:42:14.449068969 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 1] Operations flushed, joining watchdog thread.
[rank2]:[I822 23:42:14.449071520 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 2] ProcessGroupNCCL destructor entered.
[rank2]:[I822 23:42:14.449087241 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 23:42:14.449112064 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 2] Starting to destroy process group, flushing operations.
[rank2]:[I822 23:42:14.449115391 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 2] Operations flushed, joining watchdog thread.
[rank1]:[I822 23:42:14.449125336 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 23:42:14.449129253 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 1] Destroy complete.
[rank1]:[I822 23:42:14.449147863 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL destructor entered.
[rank1]:[I822 23:42:14.449167993 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 23:42:14.449169950 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 23:42:14.449175556 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 2] Destroy complete.
[rank2]:[I822 23:42:14.449191570 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL destructor entered.
[rank2]:[I822 23:42:14.449208624 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 23:42:14.459151069 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 4] ProcessGroupNCCL destructor entered.
[rank3]:[I822 23:42:14.459157160 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 3] ProcessGroupNCCL destructor entered.
[rank4]:[I822 23:42:14.459168549 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 4] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 23:42:14.459173309 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 23:42:14.459186621 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 4] Starting to destroy process group, flushing operations.
[rank4]:[I822 23:42:14.459189576 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 4] Operations flushed, joining watchdog thread.
[rank3]:[I822 23:42:14.459194801 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 3] Starting to destroy process group, flushing operations.
[rank3]:[I822 23:42:14.459199459 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 3] Operations flushed, joining watchdog thread.
[rank5]:[I822 23:42:14.459223343 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 5] ProcessGroupNCCL destructor entered.
[rank4]:[I822 23:42:14.459236901 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 4] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 23:42:14.459240294 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 4] Destroy complete.
[rank5]:[I822 23:42:14.459242087 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 5] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 23:42:14.459247753 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 23:42:14.459251989 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 3] Destroy complete.
[rank4]:[I822 23:42:14.459253588 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 4] ProcessGroupNCCL destructor entered.
[rank6]:[I822 23:42:14.459251439 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 6] ProcessGroupNCCL destructor entered.
[rank4]:[I822 23:42:14.459263759 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 4] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 23:42:14.459263958 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 6] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 23:42:14.459268695 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL destructor entered.
[rank5]:[I822 23:42:14.459267610 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 5] Starting to destroy process group, flushing operations.
[rank5]:[I822 23:42:14.459272869 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 5] Operations flushed, joining watchdog thread.
[rank6]:[I822 23:42:14.459281850 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 6] Starting to destroy process group, flushing operations.
[rank6]:[I822 23:42:14.459284905 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 6] Operations flushed, joining watchdog thread.
[rank3]:[I822 23:42:14.459298715 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 23:42:14.459311267 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 7] ProcessGroupNCCL destructor entered.
[rank5]:[I822 23:42:14.459329648 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 5] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 23:42:14.459334609 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 7] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 23:42:14.459336505 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 6] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 23:42:14.459337723 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 5] Destroy complete.
[rank6]:[I822 23:42:14.459340526 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 6] Destroy complete.
[rank6]:[I822 23:42:14.459354057 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 6] ProcessGroupNCCL destructor entered.
[rank7]:[I822 23:42:14.459360041 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 7] Starting to destroy process group, flushing operations.
[rank5]:[I822 23:42:14.459361383 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 5] ProcessGroupNCCL destructor entered.
[rank7]:[I822 23:42:14.459363494 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 7] Operations flushed, joining watchdog thread.
[rank6]:[I822 23:42:14.459367233 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 6] ProcessGroupNCCL heart beat monitor thread joined.
[rank5]:[I822 23:42:14.459378484 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 5] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 23:42:14.459426595 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 7] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 23:42:14.459436320 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 7] Destroy complete.
[rank7]:[I822 23:42:14.459453890 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 7] ProcessGroupNCCL destructor entered.
[rank7]:[I822 23:42:14.459469803 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 7] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 23:42:14.494059221 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 23:42:14.494071643 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 23:42:14.494143612 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 23:42:14.494149758 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 0] Destroy complete.
[I822 23:42:14.494198420 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL destructor entered.
[I822 23:42:14.494217136 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[I822 23:42:14.494269156 TCPStoreLibUvBackend.cpp:130] [c10d - debug] Read callback failed. code:-4095 name:EOF desc:end of file
[I822 23:42:14.494347633 TCPStoreLibUvBackend.cpp:1105] [c10d - debug] Store exit requested

[I822 23:42:14.494352813 TCPStoreLibUvBackend.cpp:1181] [c10d - debug] UV main loop done: res:1
[I822 23:42:14.494355367 TCPStoreLibUvBackend.cpp:1187] [c10d - debug] Walking live handles prior to closing clients
[I822 23:42:14.494359263 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 23:42:14.494362247 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 23:42:14.494364290 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 23:42:14.494366515 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 23:42:14.494368500 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 23:42:14.494370497 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 23:42:14.494372443 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 23:42:14.494374391 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 23:42:14.494429104 TCPStoreLibUvBackend.cpp:1197] [c10d - debug] Walking live handles after closing clients
[I822 23:42:14.494433406 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 23:42:14.494435420 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 23:42:14.494437400 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 23:42:14.494439392 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 23:42:14.494441306 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 23:42:14.494443198 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 23:42:14.494445100 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 23:42:14.494447015 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 23:42:14.494449670 TCPStoreLibUvBackend.cpp:1206] [c10d] uv_loop_close failed with:-16 errn:EBUSY desc:resource busy or locked
[I822 23:42:14.494478428 TCPStoreLibUvBackend.cpp:1216] [c10d] uv_loop cleanup finished.
[rank1]:[I822 23:42:14.496937569 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 1] Starting to destroy process group, flushing operations.
[rank1]:[I822 23:42:14.496952488 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 1] Operations flushed, joining watchdog thread.
[rank1]:[I822 23:42:14.497010368 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 23:42:14.497014873 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 1] Destroy complete.
[I822 23:42:14.497060892 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL destructor entered.
[I822 23:42:14.497084952 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 23:42:14.510083031 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 3] Starting to destroy process group, flushing operations.
[rank2]:[I822 23:42:14.510086143 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 2] Starting to destroy process group, flushing operations.
[rank2]:[I822 23:42:14.510093213 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 2] Operations flushed, joining watchdog thread.
[rank3]:[I822 23:42:14.510096030 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 3] Operations flushed, joining watchdog thread.
[rank2]:[I822 23:42:14.510141163 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 23:42:14.510145219 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 23:42:14.510146123 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 2] Destroy complete.
[rank3]:[I822 23:42:14.510147896 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 3] Destroy complete.
[I822 23:42:14.510184499 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL destructor entered.
[I822 23:42:14.510202940 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[I822 23:42:14.510203943 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL destructor entered.
[I822 23:42:14.510220372 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 23:42:14.522010196 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 4] Starting to destroy process group, flushing operations.
[rank4]:[I822 23:42:14.522021394 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 4] Operations flushed, joining watchdog thread.
[rank5]:[I822 23:42:14.522031956 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 5] Starting to destroy process group, flushing operations.
[rank5]:[I822 23:42:14.522039437 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 5] Operations flushed, joining watchdog thread.
[rank4]:[I822 23:42:14.522071348 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 4] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 23:42:14.522075404 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 4] Destroy complete.
[rank6]:[I822 23:42:14.522082830 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 6] Starting to destroy process group, flushing operations.
[rank6]:[I822 23:42:14.522090632 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 6] Operations flushed, joining watchdog thread.
[rank5]:[I822 23:42:14.522090780 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 5] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 23:42:14.522094176 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 5] Destroy complete.
[I822 23:42:14.522116193 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL destructor entered.
[I822 23:42:14.522138283 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL heart beat monitor thread joined.
[I822 23:42:14.522139016 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL destructor entered.
[rank6]:[I822 23:42:14.522143116 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 6] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 23:42:14.522146251 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 6] Destroy complete.
[I822 23:42:14.522154877 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 23:42:14.522169443 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 7] Starting to destroy process group, flushing operations.
[rank7]:[I822 23:42:14.522183367 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 7] Operations flushed, joining watchdog thread.
[I822 23:42:14.522183408 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL destructor entered.
[I822 23:42:14.522202051 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 23:42:14.522238376 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 7] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 23:42:14.522243330 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 7] Destroy complete.
[I822 23:42:14.522286347 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 7] ProcessGroupNCCL destructor entered.
[I822 23:42:14.522312158 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 7] ProcessGroupNCCL heart beat monitor thread joined.
|     Task      |Version|     Metric     |Value|   |Stderr|
|---------------|------:|----------------|----:|---|-----:|
|all            |       |extractive_match|  0.8|  |0.0743|
|custom:aime24:0|      1|extractive_match|  0.8|  |0.0743|

Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|| 1/1 [00:00<00:00, 47.57ba/s]
+ set +x
---- 2025-08-22T23:42:22+00:00 RUN END ----
---- 2025-08-22T23:43:31+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:43:33.953073280 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:44:11 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-22T23:44:51+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:44:53.877410134 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:45:31 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-22T23:46:29+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:46:31.674012021 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:47:09 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-22T23:48:20+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:48:22.790749335 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:49:00 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-22T23:51:00+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:51:02.789533888 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:51:40 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-22T23:54:54+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:54:56.677798889 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:55:34 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:01:14+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:01:16.829421928 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:01:54 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:07:26+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:07:28.844832716 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:08:06 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:13:39+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:13:41.732231735 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:14:19 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:19:54+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:19:56.740898241 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:20:34 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:26:07+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:26:09.685678679 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:26:47 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:32:19+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:32:21.676827915 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:32:59 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:38:38+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:38:42.826703417 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:39:23 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:45:07+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:45:09.705902364 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:45:47 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:51:23+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:51:25.637269097 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:52:03 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:57:36+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:57:38.817359125 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:58:16 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:03:46+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:03:48.724626868 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:04:26 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:10:05+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:10:07.784842649 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:10:45 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:16:19+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:16:21.704882336 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:16:59 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:22:40+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:22:42.720879940 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:23:20 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:28:51+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:28:53.778292991 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:29:31 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:35:05+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:35:07.744334225 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:35:45 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:41:20+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:41:22.762275910 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:42:00 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:47:38+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:47:41.282904159 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:48:19 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:53:58+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:54:00.785420379 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:54:38 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:00:10+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:00:12.953995475 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:00:50 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:06:24+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:06:26.700122332 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:07:04 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:12:36+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:12:38.677564549 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:13:16 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:18:49+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:18:51.716230334 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:19:29 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:25:02+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:25:04.805029254 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:25:42 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:31:15+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:31:17.709880524 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:31:55 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:37:37+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:37:39.723064010 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:38:17 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:43:53+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:43:55.653387842 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:44:33 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:50:08+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:50:10.784833999 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:50:48 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:56:19+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:56:21.654714454 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:56:59 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:02:34+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:02:36.602674183 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:03:14 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:08:44+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:08:46.475244026 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:09:23 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:15:07+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:15:09.604891590 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:15:47 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:21:18+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:21:20.774437674 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:21:58 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:27:29+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:27:31.730115883 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:28:09 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:33:43+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:33:45.715380508 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:34:23 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:40:02+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:40:04.628064509 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:40:42 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:46:21+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:46:23.611405904 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:47:01 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:52:38+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:52:40.691585903 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:53:18 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:58:48+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:58:50.553527473 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:59:28 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:04:58+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:05:00.581864639 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:05:38 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:11:18+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:11:20.617148890 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:11:58 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:17:33+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:17:35.486179274 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:18:12 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:23:45+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:23:47.590957150 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:24:25 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:29:58+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:30:00.604074569 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:30:38 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:36:11+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:36:13.806999958 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:36:51 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:42:27+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:42:29.683688826 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:43:07 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:48:41+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:48:43.659864425 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:49:21 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:54:55+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:54:57.598906948 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:55:35 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:01:14+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:01:16.702039757 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:01:54 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:07:37+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:07:39.650623941 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:08:17 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:13:57+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:13:59.585357500 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:14:37 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:20:14+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:20:16.748339710 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:20:54 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:26:28+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:26:30.624815352 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:27:08 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:32:47+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:32:49.652649411 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:33:27 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:39:01+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:39:03.618444506 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:39:40 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:45:12+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:45:14.720385265 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:45:52 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:51:23+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:51:25.697888533 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:52:03 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:57:36+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:57:38.646600619 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:58:16 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:03:57+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:03:59.622219506 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:04:37 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:10:16+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:10:18.628583893 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:10:56 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:16:28+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:16:30.718076028 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:17:08 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:22:42+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:22:44.714373142 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:23:22 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:28:56+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:28:58.660704651 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:29:36 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:35:14+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:35:16.726654985 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:35:54 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:41:25+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:41:27.637997183 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:42:05 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:47:46+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:47:48.712368440 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:48:26 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:54:05+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:54:07.741964581 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:54:45 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:00:23+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:00:25.769608535 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:01:03 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:06:39+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:06:41.706685947 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:07:19 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:12:58+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:13:00.732889333 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:13:38 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:19:16+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:19:18.724813478 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:19:56 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:25:28+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:25:30.682943499 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:26:08 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:31:41+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:31:43.690591972 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:32:21 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:37:58+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:38:00.669362063 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:38:38 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:44:18+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:44:20.719277513 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:45:01 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:50:40+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:50:42.776428591 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:51:20 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:56:53+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:56:55.748417631 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:57:33 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:03:14+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:03:16.866628320 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:03:54 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:09:31+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:09:33.797253290 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:10:11 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:15:47+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:15:49.771127556 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:16:27 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:22:06+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:22:08.673065450 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:22:46 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:28:27+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:28:29.628501711 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:29:07 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:34:39+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:34:41.746468495 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:35:19 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:41:00+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:41:02.785011992 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:41:40 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:47:23+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:47:25.757434810 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:48:03 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:53:43+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:53:45.609552993 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:54:23 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:00:03+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:00:05.709450011 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:00:43 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:06:14+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:06:16.574073620 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:06:54 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:12:31+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:12:33.526248058 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:13:11 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:18:44+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:18:46.465544363 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:19:23 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:24:58+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:25:00.487086826 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:25:38 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:31:11+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:31:13.503265750 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:31:50 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:37:29+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:37:31.558257040 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:38:09 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:43:45+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:43:47.499041400 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:44:25 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:49:56+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:49:58.716491967 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:50:36 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:56:16+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:56:18.639103736 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:56:56 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T10:02:29+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 10:02:31.696354049 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 10:03:09 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T10:08:39+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 10:08:41.781360149 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 10:09:19 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T10:14:51+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 10:14:53.712030886 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 10:15:31 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T10:21:06+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 10:21:08.666124257 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 10:21:47 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T10:27:26+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 10:27:28.630731363 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 10:28:06 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T10:33:48+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 10:33:50.611309947 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 10:34:28 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T10:40:10+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 10:40:12.687414013 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 10:40:50 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T10:46:25+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 10:46:27.563476533 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 10:47:05 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T10:52:40+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 10:52:42.731200450 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 10:53:20 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T10:58:53+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 10:58:55.769560883 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 10:59:33 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T11:05:14+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 11:05:16.787630688 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 11:05:54 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T11:11:28+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 11:11:30.661445597 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 11:12:08 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T11:17:47+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 11:17:49.792495613 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 11:18:27 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T11:24:06+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 11:24:08.755140403 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 11:24:46 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T11:30:21+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 11:30:23.748744503 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 11:31:01 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T11:36:32+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 11:36:34.850535911 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 11:37:12 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T11:42:46+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 11:42:48.712597628 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 11:43:26 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T11:49:02+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 11:49:04.678771628 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 11:49:42 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T11:55:11+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 11:55:13.851781644 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 11:55:51 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T12:01:32+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 12:01:34.805091556 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 12:02:12 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T12:07:52+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 12:07:54.772867315 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 12:08:32 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T12:14:11+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 12:14:13.633882277 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 12:14:51 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T12:20:22+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 12:20:24.707193865 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 12:21:02 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T12:26:38+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 12:26:40.654613992 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 12:27:18 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T12:32:47+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 12:32:49.765604567 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 12:33:27 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T12:38:59+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 12:39:01.690521885 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 12:39:39 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T12:45:13+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 12:45:15.807047930 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 12:45:53 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T12:51:24+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 12:51:26.762997354 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 12:52:04 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T12:57:35+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 12:57:37.718763235 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 12:58:15 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T13:03:48+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 13:03:50.644715594 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 13:04:28 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T13:10:05+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 13:10:07.756040548 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 13:10:45 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T13:16:20+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 13:16:22.647102559 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 13:17:00 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T13:22:42+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 13:22:44.674840874 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 13:23:22 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T13:28:57+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 13:28:59.660343745 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 13:29:37 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T13:35:13+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 13:35:15.827902206 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 13:35:53 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T13:41:28+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 13:41:30.749392458 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 13:42:08 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T13:47:40+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 13:47:42.631094982 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 13:48:20 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T13:53:51+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 13:53:53.758229766 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 13:54:31 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T14:00:12+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 14:00:14.864417709 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 14:00:52 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T14:06:25+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 14:06:27.689470721 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 14:07:05 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T14:12:34+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 14:12:36.830616622 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 14:13:14 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T14:18:53+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 14:18:55.629688588 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 14:19:33 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T14:25:13+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 14:25:15.826151783 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 14:25:53 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T14:31:36+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 14:31:38.800474509 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 14:32:16 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T14:37:53+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 14:37:55.814241342 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 14:38:33 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T14:44:16+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 14:44:18.661003817 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 14:44:56 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T14:50:29+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 14:50:31.725690449 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 14:51:09 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T14:56:51+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 14:56:53.704052643 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 14:57:31 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T15:03:05+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 15:03:07.650020516 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 15:03:46 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T15:09:25+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 15:09:27.700901333 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 15:10:05 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T15:15:45+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 15:15:47.780241138 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 15:16:25 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T15:22:00+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 15:22:02.714717746 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 15:22:40 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T15:28:10+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 15:28:12.672675862 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 15:28:50 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T15:34:29+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 15:34:31.687310685 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 15:35:09 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T15:40:42+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 15:40:44.785161730 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 15:41:22 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T15:46:58+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 15:47:00.906417220 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 15:47:38 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T15:53:09+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 15:53:11.780721138 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 15:53:49 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T15:59:28+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 15:59:30.762408304 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 16:00:08 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T16:05:39+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 16:05:41.700513009 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 16:06:19 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T16:11:51+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-8B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 16:11:53.701643610 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 16:12:31 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
