---- 2025-08-22T08:54:09+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 08:54:20.629153872 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 08:55:32 [__init__.py:244] Automatically detected platform cuda.
Using seed: 0
[I822 08:56:35.900804421 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 08:57:20 [__init__.py:244] Automatically detected platform cuda.
[I822 08:57:32.784271526 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:57:32.787717400 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:57:32.787724327 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:57:32.787731685 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:57:32.787734095 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:57:32.787735315 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:57:32.787745495 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:57:32.788427708 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 08:58:35 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 08:58:35 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 08:58:35 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 08:58:35 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 08:58:35 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 08:58:35 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 08:58:35 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 08:58:35 [__init__.py:244] Automatically detected platform cuda.
[I822 08:58:56.718141639 TCPStore.cpp:274] [c10d - debug] The server has started on port = 39039.
[I822 08:58:56.718161972 TCPStoreLibUvBackend.cpp:1178] [c10d - debug] Uv main loop running
[I822 08:58:56.718285289 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 39039).
[I822 08:58:56.720428104 socket.cpp:946] [c10d] The client socket has connected to [localhost]:39039 on SocketImpl(fd=124, addr=[localhost]:56562, remote=[localhost]:39039).
[I822 08:58:56.724454517 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:39039
[I822 08:58:56.960086083 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 39039).
[I822 08:58:56.961945101 socket.cpp:946] [c10d] The client socket has connected to [localhost]:39039 on SocketImpl(fd=116, addr=[localhost]:56570, remote=[localhost]:39039).
[I822 08:58:56.965503593 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:39039
[I822 08:58:56.968455062 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 39039).
[I822 08:58:56.972148726 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL initialization options: size: 8, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 08:58:56.972164345 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 08:58:56.970482050 socket.cpp:946] [c10d] The client socket has connected to [localhost]:39039 on SocketImpl(fd=116, addr=[localhost]:56582, remote=[localhost]:39039).
[I822 08:58:56.973920083 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:39039
[rank3]:[I822 08:58:56.976963711 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL initialization options: size: 8, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank3]:[I822 08:58:56.976973431 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 08:58:56.986912864 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 39039).
[I822 08:58:56.988821076 socket.cpp:946] [c10d] The client socket has connected to [localhost]:39039 on SocketImpl(fd=116, addr=[localhost]:56594, remote=[localhost]:39039).
[I822 08:58:56.991858834 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:39039
[I822 08:58:57.018900128 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL initialization options: size: 8, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 08:58:57.018913798 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 08:58:57.018918471 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL initialization options: size: 8, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 08:58:57.018927705 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 08:58:57.019225993 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 4] ProcessGroupNCCL initialization options: size: 8, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank2]:[I822 08:58:57.019228789 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL initialization options: size: 8, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank2]:[I822 08:58:57.019234405 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 08:58:57.019234595 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 08:58:57.097980716 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 39039).
[I822 08:58:57.101119424 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 39039).
[I822 08:58:57.100133353 socket.cpp:946] [c10d] The client socket has connected to [localhost]:39039 on SocketImpl(fd=116, addr=[localhost]:56598, remote=[localhost]:39039).
[I822 08:58:57.102615508 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:39039
[I822 08:58:57.103757689 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 39039).
[I822 08:58:57.104252297 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL initialization options: size: 8, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 08:58:57.104265718 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 08:58:57.104622674 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL initialization options: size: 8, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank1]:[I822 08:58:57.104630945 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 08:58:57.102624880 socket.cpp:946] [c10d] The client socket has connected to [localhost]:39039 on SocketImpl(fd=116, addr=[localhost]:56610, remote=[localhost]:39039).
[I822 08:58:57.105227037 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:39039
[I822 08:58:57.105740010 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 7] ProcessGroupNCCL initialization options: size: 8, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 08:58:57.105751963 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 7] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 08:58:57.106121552 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 7] ProcessGroupNCCL initialization options: size: 8, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank7]:[I822 08:58:57.106130527 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 7] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 08:58:57.106537094 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 39039).
[I822 08:58:57.105414447 socket.cpp:946] [c10d] The client socket has connected to [localhost]:39039 on SocketImpl(fd=116, addr=[localhost]:56614, remote=[localhost]:39039).
[I822 08:58:57.108068486 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:39039
[I822 08:58:57.109564966 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL initialization options: size: 8, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 08:58:57.109577676 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 08:58:57.109929262 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 6] ProcessGroupNCCL initialization options: size: 8, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank6]:[I822 08:58:57.109937651 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 08:58:57.108097589 socket.cpp:946] [c10d] The client socket has connected to [localhost]:39039 on SocketImpl(fd=116, addr=[localhost]:56624, remote=[localhost]:39039).
[I822 08:58:57.110714896 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:39039
[I822 08:58:57.111184951 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL initialization options: size: 8, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 08:58:57.111195701 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 08:58:57.111550722 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 5] ProcessGroupNCCL initialization options: size: 8, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank5]:[I822 08:58:57.111558744 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 08:58:57.120153213 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL initialization options: size: 8, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 08:58:57.120166044 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 08:58:57.120475633 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL initialization options: size: 8, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank0]:[I822 08:58:57.120483359 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 08:58:57.233032635 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL initialization options: size: 8, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank3]:[I822 08:58:57.233035373 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL initialization options: size: 8, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank7]:[I822 08:58:57.233038220 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL initialization options: size: 8, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank0]:[I822 08:58:57.233038956 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL initialization options: size: 8, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank3]:[I822 08:58:57.233043354 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 08:58:57.233044430 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 08:58:57.233046738 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 08:58:57.233046204 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 08:58:57.233082732 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL initialization options: size: 8, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank5]:[I822 08:58:57.233081642 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL initialization options: size: 8, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank4]:[I822 08:58:57.233088510 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL initialization options: size: 8, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank6]:[I822 08:58:57.233087912 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL initialization options: size: 8, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank5]:[I822 08:58:57.233091159 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 08:58:57.233091029 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 08:58:57.233097462 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 08:58:57.233097464 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
NCCL version 2.26.2+cuda12.2
libfabric:113:1755853137::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:113:1755853137::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:116:1755853137::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:117:1755853137::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:116:1755853137::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:117:1755853137::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:119:1755853137::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:119:1755853137::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:114:1755853137::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:114:1755853137::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:112:1755853137::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:112:1755853137::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:118:1755853137::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:118:1755853137::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:115:1755853137::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:115:1755853137::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:119:1755853137::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:113:1755853137::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:118:1755853137::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:117:1755853137::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:112:1755853137::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:119:1755853137::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:114:1755853137::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:116:1755853137::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:115:1755853137::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:113:1755853137::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:117:1755853137::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:118:1755853137::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:112:1755853137::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:114:1755853137::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:115:1755853137::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:116:1755853137::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
[rank2]:[I822 09:00:29.942561994 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 9
[rank2]:[I822 09:00:29.942599213 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 09:00:29.942586247 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 11
[rank4]:[I822 09:00:29.942589727 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 13 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 13
[rank3]:[I822 09:00:29.942612451 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 09:00:29.942615827 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 13 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 09:00:29.942615184 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 7
[rank6]:[I822 09:00:29.942628103 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 17 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 17
[rank5]:[I822 09:00:29.942631507 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 15 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 15
[rank0]:[I822 09:00:29.942624327 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 5
[rank1]:[I822 09:00:29.942645178 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 09:00:29.942653348 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 17 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 09:00:29.942654516 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 15 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 09:00:29.942658111 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 09:00:29.942652731 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 19 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 19
[rank7]:[I822 09:00:29.942678406 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 19 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 09:00:29.944145215 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 25 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 25
[rank6]:[I822 09:00:29.944147846 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 33 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 33
[rank3]:[I822 09:00:29.944148819 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 27 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 27
[rank5]:[I822 09:00:29.944150605 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 31 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 31
[rank6]:[I822 09:00:29.944155646 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 33 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 09:00:29.944155979 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 25 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 09:00:29.944156429 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 27 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 09:00:29.944158607 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 31 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 09:00:29.944164048 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 35 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 35
[rank7]:[I822 09:00:29.944171806 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 35 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 09:00:29.944249412 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 29 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 29
[rank4]:[I822 09:00:29.944265302 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 29 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 09:00:29.944398889 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 23 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 23
[rank0]:[I822 09:00:29.944400411 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 21 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 21
[rank0]:[I822 09:00:29.944408551 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 21 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 09:00:29.944412628 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 23 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 09:00:29.945194804 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 6] ProcessGroupNCCL initialization options: size: 8, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank6]:[I822 09:00:29.945204589 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 09:00:29.945210393 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 7] ProcessGroupNCCL initialization options: size: 8, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank7]:[I822 09:00:29.945217503 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 7] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 09:00:29.945233299 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 5] ProcessGroupNCCL initialization options: size: 8, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank5]:[I822 09:00:29.945240681 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 09:00:29.945247647 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 4] ProcessGroupNCCL initialization options: size: 8, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank4]:[I822 09:00:29.945254934 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 09:00:29.945290592 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 3] ProcessGroupNCCL initialization options: size: 8, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank2]:[I822 09:00:29.945294292 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 2] ProcessGroupNCCL initialization options: size: 8, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank2]:[I822 09:00:29.945301514 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 09:00:29.945301143 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 09:00:29.945749841 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 0] ProcessGroupNCCL initialization options: size: 8, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank1]:[I822 09:00:29.945751582 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 1] ProcessGroupNCCL initialization options: size: 8, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank1]:[I822 09:00:29.945759290 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 09:00:29.945763122 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[1;36m(VllmWorker rank=3 pid=115)[0;0m [1;36m(VllmWorker rank=2 pid=114)[0;0m [1;36m(VllmWorker rank=4 pid=116)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=0 pid=112)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=7 pid=119)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=1 pid=113)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=6 pid=118)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:01,  1.37it/s]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:13<00:07,  7.65s/it]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:27<00:00, 10.51s/it]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:27<00:00,  9.05s/it]
[1;36m(VllmWorker rank=0 pid=112)[0;0m 
[rank4]:[I822 09:02:52.467396239 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 4] Using non-blocking mode: 0
[rank7]:[I822 09:02:52.467404144 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 7] Using non-blocking mode: 0
[rank1]:[I822 09:02:52.467397260 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 1] Using non-blocking mode: 0
[rank5]:[I822 09:02:52.467410706 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 5] Using non-blocking mode: 0
[rank2]:[I822 09:02:52.467416425 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 2] Using non-blocking mode: 0
[rank6]:[I822 09:02:52.467419130 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 6] Using non-blocking mode: 0
[rank0]:[I822 09:02:52.467424663 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 0] Using non-blocking mode: 0
[rank3]:[I822 09:02:52.467435609 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 3] Using non-blocking mode: 0
[rank0]:[I822 09:02:52.467705448 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL broadcast unique ID through store took 0.025728 ms
[rank0]:[I822 09:02:52.467742632 NCCLUtils.cpp:75] Rank 0: creating NCCL communicator with mode: blocking
[rank1]:[I822 09:02:52.467897199 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL broadcast unique ID through store took 0.452189 ms
[rank1]:[I822 09:02:52.467920952 NCCLUtils.cpp:75] Rank 1: creating NCCL communicator with mode: blocking
[rank6]:[I822 09:02:52.467910829 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL broadcast unique ID through store took 0.458259 ms
[rank6]:[I822 09:02:52.467934627 NCCLUtils.cpp:75] Rank 6: creating NCCL communicator with mode: blocking
[rank4]:[I822 09:02:52.467936489 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL broadcast unique ID through store took 0.47783 ms
[rank5]:[I822 09:02:52.467949113 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL broadcast unique ID through store took 0.504294 ms
[rank4]:[I822 09:02:52.467964306 NCCLUtils.cpp:75] Rank 4: creating NCCL communicator with mode: blocking
[rank5]:[I822 09:02:52.467970955 NCCLUtils.cpp:75] Rank 5: creating NCCL communicator with mode: blocking
[rank3]:[I822 09:02:52.467964340 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL broadcast unique ID through store took 0.497402 ms
[rank3]:[I822 09:02:52.467984565 NCCLUtils.cpp:75] Rank 3: creating NCCL communicator with mode: blocking
[rank7]:[I822 09:02:52.467993590 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL broadcast unique ID through store took 0.543 ms
[rank7]:[I822 09:02:52.468060675 NCCLUtils.cpp:75] Rank 7: creating NCCL communicator with mode: blocking
[rank2]:[I822 09:02:52.468031191 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL broadcast unique ID through store took 0.571212 ms
[rank2]:[I822 09:02:52.468082416 NCCLUtils.cpp:75] Rank 2: creating NCCL communicator with mode: blocking
[rank1]:[I822 09:02:52.992989447 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 1] NCCL_DEBUG: WARN
[rank7]:[I822 09:02:52.993067477 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 7] NCCL_DEBUG: WARN
[rank3]:[I822 09:02:52.993135763 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 3] NCCL_DEBUG: WARN
[rank0]:[I822 09:02:52.993213370 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 0] NCCL_DEBUG: WARN
[rank4]:[I822 09:02:52.993354112 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 4] NCCL_DEBUG: WARN
[rank2]:[I822 09:02:52.993388016 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 2] NCCL_DEBUG: WARN
[rank5]:[I822 09:02:52.993448046 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 5] NCCL_DEBUG: WARN
[rank6]:[I822 09:02:52.993465575 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 6] NCCL_DEBUG: WARN
[1;36m(VllmWorker rank=0 pid=112)[0;0m Capturing CUDA graph shapes:   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graph shapes:   1%|         | 1/67 [00:00<00:37,  1.78it/s]Capturing CUDA graph shapes:   3%|         | 2/67 [00:01<00:36,  1.79it/s]Capturing CUDA graph shapes:   4%|         | 3/67 [00:01<00:35,  1.79it/s]Capturing CUDA graph shapes:   6%|         | 4/67 [00:02<00:35,  1.79it/s]Capturing CUDA graph shapes:   7%|         | 5/67 [00:02<00:34,  1.79it/s]Capturing CUDA graph shapes:   9%|         | 6/67 [00:03<00:34,  1.79it/s]Capturing CUDA graph shapes:  10%|         | 7/67 [00:03<00:33,  1.79it/s]Capturing CUDA graph shapes:  12%|        | 8/67 [00:04<00:33,  1.78it/s]Capturing CUDA graph shapes:  13%|        | 9/67 [00:05<00:32,  1.78it/s]Capturing CUDA graph shapes:  15%|        | 10/67 [00:05<00:31,  1.78it/s]Capturing CUDA graph shapes:  16%|        | 11/67 [00:06<00:31,  1.79it/s]Capturing CUDA graph shapes:  18%|        | 12/67 [00:06<00:30,  1.78it/s]Capturing CUDA graph shapes:  19%|        | 13/67 [00:07<00:32,  1.65it/s]Capturing CUDA graph shapes:  21%|        | 14/67 [00:08<00:33,  1.58it/s]Capturing CUDA graph shapes:  22%|       | 15/67 [00:08<00:32,  1.59it/s]Capturing CUDA graph shapes:  24%|       | 16/67 [00:09<00:31,  1.64it/s]Capturing CUDA graph shapes:  25%|       | 17/67 [00:09<00:29,  1.68it/s]Capturing CUDA graph shapes:  27%|       | 18/67 [00:10<00:28,  1.71it/s]Capturing CUDA graph shapes:  28%|       | 19/67 [00:10<00:27,  1.73it/s]Capturing CUDA graph shapes:  30%|       | 20/67 [00:11<00:26,  1.75it/s]Capturing CUDA graph shapes:  31%|      | 21/67 [00:12<00:26,  1.76it/s]Capturing CUDA graph shapes:  33%|      | 22/67 [00:12<00:25,  1.76it/s]Capturing CUDA graph shapes:  34%|      | 23/67 [00:13<00:24,  1.77it/s]Capturing CUDA graph shapes:  36%|      | 24/67 [00:13<00:24,  1.77it/s]Capturing CUDA graph shapes:  37%|      | 25/67 [00:14<00:23,  1.77it/s]Capturing CUDA graph shapes:  39%|      | 26/67 [00:14<00:23,  1.77it/s]Capturing CUDA graph shapes:  40%|      | 27/67 [00:15<00:22,  1.77it/s]Capturing CUDA graph shapes:  42%|     | 28/67 [00:16<00:21,  1.78it/s]Capturing CUDA graph shapes:  43%|     | 29/67 [00:16<00:21,  1.78it/s]Capturing CUDA graph shapes:  45%|     | 30/67 [00:17<00:20,  1.77it/s]Capturing CUDA graph shapes:  46%|     | 31/67 [00:17<00:22,  1.63it/s]Capturing CUDA graph shapes:  48%|     | 32/67 [00:18<00:22,  1.57it/s]Capturing CUDA graph shapes:  49%|     | 33/67 [00:19<00:21,  1.57it/s]Capturing CUDA graph shapes:  51%|     | 34/67 [00:19<00:20,  1.62it/s]Capturing CUDA graph shapes:  52%|    | 35/67 [00:20<00:19,  1.66it/s]Capturing CUDA graph shapes:  54%|    | 36/67 [00:20<00:18,  1.69it/s]Capturing CUDA graph shapes:  55%|    | 37/67 [00:21<00:17,  1.72it/s]Capturing CUDA graph shapes:  57%|    | 38/67 [00:22<00:16,  1.73it/s]Capturing CUDA graph shapes:  58%|    | 39/67 [00:22<00:16,  1.74it/s]Capturing CUDA graph shapes:  60%|    | 40/67 [00:23<00:15,  1.75it/s]Capturing CUDA graph shapes:  61%|    | 41/67 [00:23<00:14,  1.74it/s]Capturing CUDA graph shapes:  63%|   | 42/67 [00:24<00:14,  1.74it/s]Capturing CUDA graph shapes:  64%|   | 43/67 [00:24<00:13,  1.75it/s]Capturing CUDA graph shapes:  66%|   | 44/67 [00:25<00:13,  1.75it/s]Capturing CUDA graph shapes:  67%|   | 45/67 [00:26<00:12,  1.75it/s]Capturing CUDA graph shapes:  69%|   | 46/67 [00:26<00:12,  1.74it/s]Capturing CUDA graph shapes:  70%|   | 47/67 [00:27<00:11,  1.74it/s]Capturing CUDA graph shapes:  72%|  | 48/67 [00:27<00:11,  1.61it/s]Capturing CUDA graph shapes:  73%|  | 49/67 [00:28<00:11,  1.53it/s]Capturing CUDA graph shapes:  75%|  | 50/67 [00:29<00:11,  1.54it/s]Capturing CUDA graph shapes:  76%|  | 51/67 [00:29<00:10,  1.58it/s]Capturing CUDA graph shapes:  78%|  | 52/67 [00:30<00:09,  1.63it/s]Capturing CUDA graph shapes:  79%|  | 53/67 [00:31<00:08,  1.67it/s]Capturing CUDA graph shapes:  81%|  | 54/67 [00:31<00:07,  1.69it/s]Capturing CUDA graph shapes:  82%| | 55/67 [00:32<00:07,  1.71it/s]Capturing CUDA graph shapes:  84%| | 56/67 [00:32<00:06,  1.72it/s]Capturing CUDA graph shapes:  85%| | 57/67 [00:33<00:05,  1.73it/s]Capturing CUDA graph shapes:  87%| | 58/67 [00:33<00:05,  1.74it/s]Capturing CUDA graph shapes:  88%| | 59/67 [00:34<00:04,  1.74it/s]Capturing CUDA graph shapes:  90%| | 60/67 [00:35<00:03,  1.75it/s]Capturing CUDA graph shapes:  91%| | 61/67 [00:35<00:03,  1.75it/s]Capturing CUDA graph shapes:  93%|| 62/67 [00:36<00:02,  1.76it/s]Capturing CUDA graph shapes:  94%|| 63/67 [00:36<00:02,  1.77it/s]Capturing CUDA graph shapes:  96%|| 64/67 [00:37<00:01,  1.78it/s]Capturing CUDA graph shapes:  97%|| 65/67 [00:37<00:01,  1.64it/s]Capturing CUDA graph shapes:  99%|| 66/67 [00:38<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|| 67/67 [00:39<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|| 67/67 [00:39<00:00,  1.70it/s]
If you want to use extended_tasks, make sure you installed their dependencies using `pip install -e .[extended_tasks]`.
Careful, the task custom|aime24 is using evaluation data to build the few shot examples.
You cannot select the number of dataset splits for a generative evaluation at the moment. Automatically inferring.
Splits:   0%|          | 0/1 [00:00<?, ?it/s]
Adding requests:   0%|          | 0/30 [00:00<?, ?it/s][AAdding requests: 100%|| 30/30 [00:00<00:00, 2550.04it/s]

Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   3%|         | 1/30 [00:27<13:16, 27.46s/it, est. speed input: 8.12 toks/s, output: 120.88 toks/s][A
Processed prompts:   7%|         | 2/30 [00:31<06:26, 13.79s/it, est. speed input: 13.79 toks/s, output: 225.41 toks/s][A
Processed prompts:  10%|         | 3/30 [00:34<03:51,  8.58s/it, est. speed input: 19.67 toks/s, output: 329.94 toks/s][A
Processed prompts:  13%|        | 4/30 [00:34<02:24,  5.55s/it, est. speed input: 23.08 toks/s, output: 441.62 toks/s][A
Processed prompts:  17%|        | 5/30 [00:36<01:39,  3.97s/it, est. speed input: 26.33 toks/s, output: 547.68 toks/s][A
Processed prompts:  20%|        | 6/30 [00:42<01:53,  4.71s/it, est. speed input: 26.06 toks/s, output: 587.54 toks/s][A
Processed prompts:  23%|       | 7/30 [00:47<01:51,  4.86s/it, est. speed input: 26.16 toks/s, output: 643.28 toks/s][A
Processed prompts:  27%|       | 8/30 [00:51<01:44,  4.73s/it, est. speed input: 27.59 toks/s, output: 707.39 toks/s][A
Processed prompts:  30%|       | 9/30 [00:58<01:49,  5.22s/it, est. speed input: 27.84 toks/s, output: 750.25 toks/s][A
Processed prompts:  33%|      | 10/30 [01:03<01:46,  5.31s/it, est. speed input: 27.78 toks/s, output: 804.14 toks/s][A
Processed prompts:  37%|      | 11/30 [01:05<01:22,  4.34s/it, est. speed input: 29.53 toks/s, output: 897.10 toks/s][A
Processed prompts:  40%|      | 12/30 [01:08<01:10,  3.89s/it, est. speed input: 30.72 toks/s, output: 978.40 toks/s][A
Processed prompts:  43%|     | 13/30 [01:23<02:02,  7.22s/it, est. speed input: 28.69 toks/s, output: 922.67 toks/s][A
Processed prompts:  47%|     | 14/30 [01:26<01:36,  6.02s/it, est. speed input: 30.40 toks/s, output: 1006.61 toks/s][A
Processed prompts:  50%|     | 15/30 [01:28<01:10,  4.69s/it, est. speed input: 32.06 toks/s, output: 1106.46 toks/s][A
Processed prompts:  53%|    | 16/30 [01:28<00:47,  3.39s/it, est. speed input: 34.50 toks/s, output: 1220.16 toks/s][A
Processed prompts:  57%|    | 17/30 [01:32<00:44,  3.39s/it, est. speed input: 35.42 toks/s, output: 1293.60 toks/s][A
Processed prompts:  60%|    | 18/30 [01:42<01:04,  5.35s/it, est. speed input: 33.50 toks/s, output: 1286.72 toks/s][A
Processed prompts:  63%|   | 19/30 [02:00<01:42,  9.29s/it, est. speed input: 29.89 toks/s, output: 1208.39 toks/s][A
Processed prompts:  67%|   | 20/30 [02:01<01:06,  6.69s/it, est. speed input: 30.90 toks/s, output: 1320.74 toks/s][A
Processed prompts:  70%|   | 21/30 [02:21<01:37, 10.78s/it, est. speed input: 27.54 toks/s, output: 1250.09 toks/s][A
Processed prompts:  73%|  | 22/30 [02:23<01:05,  8.19s/it, est. speed input: 28.32 toks/s, output: 1350.37 toks/s][A
Processed prompts:  77%|  | 23/30 [02:27<00:48,  6.87s/it, est. speed input: 28.85 toks/s, output: 1434.78 toks/s][A
Processed prompts:  80%|  | 24/30 [02:29<00:31,  5.31s/it, est. speed input: 30.33 toks/s, output: 1537.84 toks/s][A
Processed prompts:  83%| | 25/30 [02:49<00:49,  9.85s/it, est. speed input: 27.71 toks/s, output: 1473.25 toks/s][A
Processed prompts:  87%| | 26/30 [02:49<00:27,  6.95s/it, est. speed input: 30.53 toks/s, output: 1592.69 toks/s][A
Processed prompts:  90%| | 27/30 [04:17<01:33, 31.07s/it, est. speed input: 20.78 toks/s, output: 1176.82 toks/s][A
Processed prompts:  93%|| 28/30 [04:21<00:46, 23.05s/it, est. speed input: 21.34 toks/s, output: 1282.66 toks/s][A
Processed prompts: 100%|| 30/30 [04:21<00:00, 23.05s/it, est. speed input: 22.61 toks/s, output: 1533.32 toks/s][AProcessed prompts: 100%|| 30/30 [04:21<00:00,  8.71s/it, est. speed input: 22.61 toks/s, output: 1533.32 toks/s]
Splits: 100%|| 1/1 [04:21<00:00, 261.66s/it]Splits: 100%|| 1/1 [04:21<00:00, 261.66s/it]
[rank3]:[I822 09:08:00.519633466 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 3] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:08:00.519633093 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 1] Starting to destroy process group, flushing operations.
[rank2]:[I822 09:08:00.519638860 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 2] Starting to destroy process group, flushing operations.
[rank4]:[I822 09:08:00.519657253 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 4] Starting to destroy process group, flushing operations.
[rank5]:[I822 09:08:00.519681877 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 5] Starting to destroy process group, flushing operations.
[rank0]:[I822 09:08:00.519815134 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 0] Starting to destroy process group, flushing operations.
[rank7]:[I822 09:08:00.519867130 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 7] Starting to destroy process group, flushing operations.
[rank6]:[I822 09:08:00.520706327 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 6] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:08:00.521987807 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 1] Operations flushed, joining watchdog thread.
[rank5]:[I822 09:08:00.521990293 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 5] Operations flushed, joining watchdog thread.
[rank2]:[I822 09:08:00.522020799 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 2] Operations flushed, joining watchdog thread.
[rank4]:[I822 09:08:00.522039147 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 4] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:08:00.522047209 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 3] Operations flushed, joining watchdog thread.
[rank5]:[I822 09:08:00.522071995 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 5] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:08:00.522072500 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:08:00.522106531 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 09:08:00.522120656 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 4] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 09:08:00.522129192 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:08:00.522269539 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 0] Operations flushed, joining watchdog thread.
[rank7]:[I822 09:08:00.522312933 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 7] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:08:00.522385840 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 09:08:00.522398338 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 7] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 09:08:00.523107756 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 6] Operations flushed, joining watchdog thread.
[rank6]:[I822 09:08:00.523222982 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 6] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:08:00.667982705 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 0] Destroy complete.
[rank1]:[I822 09:08:00.673844017 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 1] Destroy complete.
[rank3]:[I822 09:08:00.676419995 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 3] Destroy complete.
[rank2]:[I822 09:08:00.676581972 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 2] Destroy complete.
[rank7]:[I822 09:08:00.681413869 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 7] Destroy complete.
[rank5]:[I822 09:08:00.688686947 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 5] Destroy complete.
[rank6]:[I822 09:08:00.688710409 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 6] Destroy complete.
[rank4]:[I822 09:08:00.689365620 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 4] Destroy complete.
[rank0]:[I822 09:08:00.717836185 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 09:08:00.717859135 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 09:08:00.717909169 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 5 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 09:08:00.717914311 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 5 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:08:00.717958406 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 5 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:08:00.717965715 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 5 Rank 0] Destroy complete.
[rank0]:[I822 09:08:00.717984963 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 09:08:00.717999863 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 09:08:00.737164492 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL destructor entered.
[rank1]:[I822 09:08:00.737196842 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 09:08:00.737279424 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 7 Rank 0] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:08:00.737285557 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 7 Rank 0] Operations flushed, joining watchdog thread.
[rank1]:[I822 09:08:00.737359372 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 7 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:08:00.737365893 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 7 Rank 0] Destroy complete.
[rank1]:[I822 09:08:00.737400163 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL destructor entered.
[rank1]:[I822 09:08:00.737420696 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 09:08:00.737427300 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 21 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 09:08:00.737434594 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 21 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:08:00.737498138 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 21 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:08:00.737505983 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 21 Rank 0] Destroy complete.
[rank0]:[I822 09:08:00.737521548 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 21 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 09:08:00.737544990 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 21 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 09:08:00.737938739 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:08:00.737960538 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 09:08:00.738005782 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 9 Rank 0] Starting to destroy process group, flushing operations.
[rank2]:[I822 09:08:00.738011262 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 9 Rank 0] Operations flushed, joining watchdog thread.
[rank2]:[I822 09:08:00.738068588 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 9 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:08:00.738074614 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 9 Rank 0] Destroy complete.
[rank2]:[I822 09:08:00.738093850 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:08:00.738113210 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 09:08:00.750816774 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 25 Rank 0] Starting to destroy process group, flushing operations.
[rank2]:[I822 09:08:00.750824847 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 25 Rank 0] Operations flushed, joining watchdog thread.
[rank2]:[I822 09:08:00.750871486 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 25 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:08:00.750876250 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 25 Rank 0] Destroy complete.
[rank2]:[I822 09:08:00.750888376 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 25 Rank 0] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:08:00.750924092 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 25 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:08:00.755382610 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL destructor entered.
[rank3]:[I822 09:08:00.755403964 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:08:00.755447136 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 11 Rank 0] Starting to destroy process group, flushing operations.
[rank3]:[I822 09:08:00.755452318 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 11 Rank 0] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:08:00.755506017 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 11 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 09:08:00.755511713 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 11 Rank 0] Destroy complete.
[rank3]:[I822 09:08:00.755530185 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL destructor entered.
[rank3]:[I822 09:08:00.755551693 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 09:08:00.755945220 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 23 Rank 0] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:08:00.755959968 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 23 Rank 0] Operations flushed, joining watchdog thread.
[rank1]:[I822 09:08:00.756041617 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 23 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:08:00.756047218 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 23 Rank 0] Destroy complete.
[rank1]:[I822 09:08:00.756068567 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 23 Rank 0] ProcessGroupNCCL destructor entered.
[rank1]:[I822 09:08:00.756087547 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 23 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 09:08:00.757222056 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 09:08:00.757233396 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:08:00.757290262 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:08:00.757295220 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 0] Destroy complete.
[rank4]:[I822 09:08:00.765511265 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL destructor entered.
[rank4]:[I822 09:08:00.765535728 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 09:08:00.765579891 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 13 Rank 0] Starting to destroy process group, flushing operations.
[rank4]:[I822 09:08:00.765584941 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 13 Rank 0] Operations flushed, joining watchdog thread.
[rank4]:[I822 09:08:00.765631046 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 13 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 09:08:00.765637042 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 13 Rank 0] Destroy complete.
[rank4]:[I822 09:08:00.765656032 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 13 Rank 0] ProcessGroupNCCL destructor entered.
[rank4]:[I822 09:08:00.765678163 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 13 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:08:00.765800660 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 27 Rank 0] Starting to destroy process group, flushing operations.
[rank3]:[I822 09:08:00.765808162 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 27 Rank 0] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:08:00.765853455 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 27 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 09:08:00.765857978 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 27 Rank 0] Destroy complete.
[rank3]:[I822 09:08:00.765870335 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 27 Rank 0] ProcessGroupNCCL destructor entered.
[rank3]:[I822 09:08:00.765899161 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 27 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 09:08:00.770968872 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 2] Starting to destroy process group, flushing operations.
[rank2]:[I822 09:08:00.770977553 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 2] Operations flushed, joining watchdog thread.
[rank2]:[I822 09:08:00.771037155 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:08:00.771041942 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 2] Destroy complete.
[rank1]:[I822 09:08:00.774246278 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 1] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:08:00.774260159 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 1] Operations flushed, joining watchdog thread.
[rank1]:[I822 09:08:00.774302623 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:08:00.774306967 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 1] Destroy complete.
[rank6]:[I822 09:08:00.775430912 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL destructor entered.
[rank6]:[I822 09:08:00.775455964 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 09:08:00.775500394 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 17 Rank 0] Starting to destroy process group, flushing operations.
[rank6]:[I822 09:08:00.775505626 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 17 Rank 0] Operations flushed, joining watchdog thread.
[rank6]:[I822 09:08:00.775567014 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 17 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 09:08:00.775573017 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 17 Rank 0] Destroy complete.
[rank6]:[I822 09:08:00.775591306 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 17 Rank 0] ProcessGroupNCCL destructor entered.
[rank6]:[I822 09:08:00.775609488 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 17 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 09:08:00.777087579 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 29 Rank 0] Starting to destroy process group, flushing operations.
[rank4]:[I822 09:08:00.777097561 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 29 Rank 0] Operations flushed, joining watchdog thread.
[rank4]:[I822 09:08:00.777153775 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 29 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 09:08:00.777158375 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 29 Rank 0] Destroy complete.
[rank4]:[I822 09:08:00.777172840 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 29 Rank 0] ProcessGroupNCCL destructor entered.
[rank4]:[I822 09:08:00.777191844 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 29 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:08:00.777303173 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 3] Starting to destroy process group, flushing operations.
[rank3]:[I822 09:08:00.777310763 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 3] Operations flushed, joining watchdog thread.
[rank5]:[I822 09:08:00.777341837 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL destructor entered.
[rank5]:[I822 09:08:00.777362553 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:08:00.777370713 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 09:08:00.777373498 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 3] Destroy complete.
[rank5]:[I822 09:08:00.777408010 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 15 Rank 0] Starting to destroy process group, flushing operations.
[rank5]:[I822 09:08:00.777413202 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 15 Rank 0] Operations flushed, joining watchdog thread.
[rank5]:[I822 09:08:00.777485136 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 15 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 09:08:00.777491206 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 15 Rank 0] Destroy complete.
[rank5]:[I822 09:08:00.777512356 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 15 Rank 0] ProcessGroupNCCL destructor entered.
[rank5]:[I822 09:08:00.777533209 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 15 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 09:08:00.778170716 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL destructor entered.
[rank7]:[I822 09:08:00.778205292 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 09:08:00.778273418 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 19 Rank 0] Starting to destroy process group, flushing operations.
[rank7]:[I822 09:08:00.778279064 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 19 Rank 0] Operations flushed, joining watchdog thread.
[rank7]:[I822 09:08:00.778329933 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 19 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 09:08:00.778335624 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 19 Rank 0] Destroy complete.
[rank7]:[I822 09:08:00.778365689 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 19 Rank 0] ProcessGroupNCCL destructor entered.
[rank7]:[I822 09:08:00.778382452 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 19 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 09:08:00.788339396 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 33 Rank 0] Starting to destroy process group, flushing operations.
[rank6]:[I822 09:08:00.788354243 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 33 Rank 0] Operations flushed, joining watchdog thread.
[rank6]:[I822 09:08:00.788406261 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 33 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 09:08:00.788410845 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 33 Rank 0] Destroy complete.
[rank6]:[I822 09:08:00.788448855 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 33 Rank 0] ProcessGroupNCCL destructor entered.
[rank6]:[I822 09:08:00.788473198 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 33 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank5]:[I822 09:08:00.788980905 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 31 Rank 0] Starting to destroy process group, flushing operations.
[rank5]:[I822 09:08:00.788990162 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 31 Rank 0] Operations flushed, joining watchdog thread.
[rank5]:[I822 09:08:00.789035736 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 31 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 09:08:00.789038257 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 31 Rank 0] Destroy complete.
[rank5]:[I822 09:08:00.789049447 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 31 Rank 0] ProcessGroupNCCL destructor entered.
[rank5]:[I822 09:08:00.789081679 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 31 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 09:08:00.792027314 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 35 Rank 0] Starting to destroy process group, flushing operations.
[rank7]:[I822 09:08:00.792039141 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 35 Rank 0] Operations flushed, joining watchdog thread.
[rank7]:[I822 09:08:00.792089146 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 35 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 09:08:00.792093406 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 35 Rank 0] Destroy complete.
[rank7]:[I822 09:08:00.792109367 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 35 Rank 0] ProcessGroupNCCL destructor entered.
[rank7]:[I822 09:08:00.792125247 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 35 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 09:08:00.792165908 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 4] Starting to destroy process group, flushing operations.
[rank4]:[I822 09:08:00.792180958 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 4] Operations flushed, joining watchdog thread.
[rank4]:[I822 09:08:00.792226800 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 4] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 09:08:00.792231174 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 4] Destroy complete.
[rank6]:[I822 09:08:00.799016694 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 6] Starting to destroy process group, flushing operations.
[rank6]:[I822 09:08:00.799026107 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 6] Operations flushed, joining watchdog thread.
[rank6]:[I822 09:08:00.799071078 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 6] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 09:08:00.799075336 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 6] Destroy complete.
[rank5]:[I822 09:08:00.806078277 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 5] Starting to destroy process group, flushing operations.
[rank5]:[I822 09:08:00.806089344 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 5] Operations flushed, joining watchdog thread.
[rank5]:[I822 09:08:00.806150395 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 5] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 09:08:00.806155289 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 5] Destroy complete.
[rank7]:[I822 09:08:00.810422668 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 7] Starting to destroy process group, flushing operations.
[rank7]:[I822 09:08:00.810433237 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 7] Operations flushed, joining watchdog thread.
[rank7]:[I822 09:08:00.810477630 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 7] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 09:08:00.810481763 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 7] Destroy complete.
[rank0]:[I822 09:08:00.839462275 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 09:08:00.839483714 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 09:08:00.839505441 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 09:08:00.839508307 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:08:00.839559487 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:08:00.839565167 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 0] Destroy complete.
[rank0]:[I822 09:08:00.839581650 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 09:08:00.839592314 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 09:08:00.849581118 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 2] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:08:00.849601265 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 09:08:00.849608807 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 1] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:08:00.849621285 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 2] Starting to destroy process group, flushing operations.
[rank2]:[I822 09:08:00.849624943 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 2] Operations flushed, joining watchdog thread.
[rank1]:[I822 09:08:00.849635478 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:08:00.849648791 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 3] ProcessGroupNCCL destructor entered.
[rank1]:[I822 09:08:00.849658997 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 1] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:08:00.849662184 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 1] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:08:00.849665423 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 09:08:00.849676163 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:08:00.849683643 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 2] Destroy complete.
[rank3]:[I822 09:08:00.849684877 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 3] Starting to destroy process group, flushing operations.
[rank3]:[I822 09:08:00.849688631 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 3] Operations flushed, joining watchdog thread.
[rank2]:[I822 09:08:00.849698407 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL destructor entered.
[rank4]:[I822 09:08:00.849701997 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 4] ProcessGroupNCCL destructor entered.
[rank4]:[I822 09:08:00.849723090 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 4] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 09:08:00.849723561 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:08:00.849723953 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 09:08:00.849727435 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 1] Destroy complete.
[rank3]:[I822 09:08:00.849736897 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 09:08:00.849741962 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 3] Destroy complete.
[rank1]:[I822 09:08:00.849746140 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL destructor entered.
[rank5]:[I822 09:08:00.849748267 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 5] ProcessGroupNCCL destructor entered.
[rank4]:[I822 09:08:00.849752759 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 4] Starting to destroy process group, flushing operations.
[rank4]:[I822 09:08:00.849756499 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 4] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:08:00.849758264 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL destructor entered.
[rank6]:[I822 09:08:00.849757741 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 6] ProcessGroupNCCL destructor entered.
[rank5]:[I822 09:08:00.849764470 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 5] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:08:00.849774848 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 09:08:00.849777747 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 6] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 09:08:00.849779764 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank5]:[I822 09:08:00.849783112 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 5] Starting to destroy process group, flushing operations.
[rank5]:[I822 09:08:00.849786274 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 5] Operations flushed, joining watchdog thread.
[rank6]:[I822 09:08:00.849802384 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 6] Starting to destroy process group, flushing operations.
[rank6]:[I822 09:08:00.849809302 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 6] Operations flushed, joining watchdog thread.
[rank4]:[I822 09:08:00.849815745 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 4] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 09:08:00.849819671 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 4] Destroy complete.
[rank5]:[I822 09:08:00.849830263 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 5] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 09:08:00.849838351 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 4] ProcessGroupNCCL destructor entered.
[rank5]:[I822 09:08:00.849838836 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 5] Destroy complete.
[rank4]:[I822 09:08:00.849853481 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 4] ProcessGroupNCCL heart beat monitor thread joined.
[rank5]:[I822 09:08:00.849856541 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 5] ProcessGroupNCCL destructor entered.
[rank6]:[I822 09:08:00.849868486 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 6] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 09:08:00.849870310 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 5] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 09:08:00.849872934 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 6] Destroy complete.
[rank6]:[I822 09:08:00.849890799 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 6] ProcessGroupNCCL destructor entered.
[rank6]:[I822 09:08:00.849903650 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 6] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 09:08:00.859852289 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 7] ProcessGroupNCCL destructor entered.
[rank7]:[I822 09:08:00.859874075 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 7] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 09:08:00.859896925 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 7] Starting to destroy process group, flushing operations.
[rank7]:[I822 09:08:00.859901965 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 7] Operations flushed, joining watchdog thread.
[rank7]:[I822 09:08:00.859960164 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 7] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 09:08:00.859966112 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 7] Destroy complete.
[rank7]:[I822 09:08:00.859983855 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 7] ProcessGroupNCCL destructor entered.
[rank7]:[I822 09:08:00.860009465 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 7] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 09:08:00.888908544 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 09:08:00.888920200 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:08:00.888960420 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:08:00.888964901 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 0] Destroy complete.
[I822 09:08:00.889000290 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL destructor entered.
[I822 09:08:00.889019718 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[I822 09:08:00.889065272 TCPStoreLibUvBackend.cpp:130] [c10d - debug] Read callback failed. code:-4095 name:EOF desc:end of file
[I822 09:08:00.889139935 TCPStoreLibUvBackend.cpp:1105] [c10d - debug] Store exit requested

[I822 09:08:00.889146608 TCPStoreLibUvBackend.cpp:1181] [c10d - debug] UV main loop done: res:1
[I822 09:08:00.889149748 TCPStoreLibUvBackend.cpp:1187] [c10d - debug] Walking live handles prior to closing clients
[I822 09:08:00.889153265 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:08:00.889156008 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:08:00.889158011 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:08:00.889160211 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:08:00.889162284 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:08:00.889164275 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:08:00.889166314 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:08:00.889168263 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:08:00.889227321 TCPStoreLibUvBackend.cpp:1197] [c10d - debug] Walking live handles after closing clients
[I822 09:08:00.889231386 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:08:00.889233601 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:08:00.889235575 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:08:00.889237533 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:08:00.889239511 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:08:00.889241435 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:08:00.889243355 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:08:00.889245269 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:08:00.889247923 TCPStoreLibUvBackend.cpp:1206] [c10d] uv_loop_close failed with:-16 errn:EBUSY desc:resource busy or locked
[I822 09:08:00.889275673 TCPStoreLibUvBackend.cpp:1216] [c10d] uv_loop cleanup finished.
[rank2]:[I822 09:08:00.903451071 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 2] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:08:00.903455920 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 1] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:08:00.903463363 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 1] Operations flushed, joining watchdog thread.
[rank2]:[I822 09:08:00.903463361 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 2] Operations flushed, joining watchdog thread.
[rank2]:[I822 09:08:00.903511201 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:08:00.903515672 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 2] Destroy complete.
[rank1]:[I822 09:08:00.903521518 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:08:00.903525062 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 1] Destroy complete.
[I822 09:08:00.903551470 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL destructor entered.
[I822 09:08:00.903570596 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL destructor entered.
[I822 09:08:00.903571429 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[I822 09:08:00.903591334 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:08:00.913507271 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 3] Starting to destroy process group, flushing operations.
[rank3]:[I822 09:08:00.913516819 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 3] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:08:00.913561060 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 09:08:00.913566285 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 3] Destroy complete.
[I822 09:08:00.913600288 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL destructor entered.
[I822 09:08:00.913617625 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 09:08:00.923591739 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 4] Starting to destroy process group, flushing operations.
[rank4]:[I822 09:08:00.923606162 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 4] Operations flushed, joining watchdog thread.
[rank4]:[I822 09:08:00.923662167 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 4] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 09:08:00.923666583 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 4] Destroy complete.
[I822 09:08:00.923710075 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL destructor entered.
[I822 09:08:00.923731915 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 09:08:00.933586980 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 6] Starting to destroy process group, flushing operations.
[rank6]:[I822 09:08:00.933600202 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 6] Operations flushed, joining watchdog thread.
[rank6]:[I822 09:08:00.933657474 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 6] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 09:08:00.933661996 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 6] Destroy complete.
[rank7]:[I822 09:08:00.933668927 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 7] Starting to destroy process group, flushing operations.
[rank7]:[I822 09:08:00.933683528 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 7] Operations flushed, joining watchdog thread.
[I822 09:08:00.933705757 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL destructor entered.
[I822 09:08:00.933730421 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 09:08:00.933736274 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 7] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 09:08:00.933739247 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 7] Destroy complete.
[I822 09:08:00.933785381 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 7] ProcessGroupNCCL destructor entered.
[I822 09:08:00.933806919 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 7] ProcessGroupNCCL heart beat monitor thread joined.
[rank5]:[I822 09:08:00.939186514 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 5] Starting to destroy process group, flushing operations.
[rank5]:[I822 09:08:00.939197567 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 5] Operations flushed, joining watchdog thread.
[rank5]:[I822 09:08:00.939246323 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 5] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 09:08:00.939251771 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 5] Destroy complete.
[I822 09:08:00.939286895 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL destructor entered.
[I822 09:08:00.939302231 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL heart beat monitor thread joined.
|     Task      |Version|     Metric     |Value |   |Stderr|
|---------------|------:|----------------|-----:|---|-----:|
|all            |       |extractive_match|0.6333|  |0.0895|
|custom:aime24:0|      1|extractive_match|0.6333|  |0.0895|

Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|| 1/1 [00:00<00:00, 29.09ba/s]
+ set +x
---- 2025-08-22T09:08:09+00:00 RUN END ----
---- 2025-08-22T23:43:41+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:43:43.667473993 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:44:21 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-22T23:45:05+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:45:07.363442427 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:45:45 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-22T23:46:39+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:46:42.265696431 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:47:20 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-22T23:48:39+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:48:41.332388132 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:49:19 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-22T23:51:11+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:51:14.249574663 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:51:52 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-22T23:55:11+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:55:13.394624238 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:55:51 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:01:30+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:01:32.298412787 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:02:10 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:07:40+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:07:42.452693911 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:08:20 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:13:59+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:14:01.345936850 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:14:39 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:20:21+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:20:23.417727722 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:21:01 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:26:36+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:26:38.442572012 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:27:16 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:32:55+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:32:57.401361085 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:33:35 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:39:11+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:39:15.948198028 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:39:55 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:45:35+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:45:40.093480859 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:46:18 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:51:50+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:51:52.327178545 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:52:30 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:58:09+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:58:11.383611159 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:58:49 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:04:24+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:04:26.399190302 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:05:04 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:10:38+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:10:40.379797388 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:11:18 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:16:48+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:16:50.270124484 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:17:28 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:23:03+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:23:06.209143744 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:23:44 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:29:20+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:29:22.279993929 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:30:00 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:35:31+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:35:33.384543823 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:36:11 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:41:42+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:41:44.314809289 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:42:22 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:48:00+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:48:02.354663900 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:48:40 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:54:11+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:54:14.179981534 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:54:52 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:00:31+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:00:33.382462841 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:01:11 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:06:45+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:06:47.342528101 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:07:25 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:13:05+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:13:07.415445116 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:13:45 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:19:22+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:19:24.396431807 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:20:02 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:25:37+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:25:39.334749421 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:26:17 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:31:54+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:31:56.439690486 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:32:34 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:38:09+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:38:11.420317236 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:38:49 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:44:24+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:44:26.377814565 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:45:04 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:50:47+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:50:49.463382487 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:51:27 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:57:00+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:57:02.414998453 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:57:40 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:03:11+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:03:13.489440385 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:03:51 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:09:28+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:09:30.372669922 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:10:08 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:15:38+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:15:40.321447797 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:16:18 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:21:58+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:22:00.362449631 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:22:38 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:28:09+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:28:11.409885832 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:28:49 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:34:27+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:34:29.425742277 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:35:07 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:40:37+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:40:39.391442521 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:41:17 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:46:59+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:47:01.405120887 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:47:39 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:53:20+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:53:22.376151395 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:54:00 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:59:42+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:59:44.590564089 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:00:22 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:05:55+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:05:57.419933365 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:06:35 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:12:10+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:12:12.381064468 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:12:50 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:18:31+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:18:33.406379901 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:19:11 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:24:46+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:24:48.444484512 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:25:26 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:30:58+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:31:00.313480380 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:31:38 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:37:16+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:37:18.365485388 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:37:56 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:43:36+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:43:38.446600919 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:44:16 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:49:47+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:49:49.391677425 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:50:27 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:55:57+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:55:59.301690427 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:56:37 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:02:07+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:02:09.418198749 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:02:47 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:08:22+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:08:24.444616083 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:09:02 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:14:34+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:14:36.420825466 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:15:14 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:20:46+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:20:48.330363739 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:21:26 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:26:56+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:26:58.340127932 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:27:36 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:33:14+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:33:16.393180103 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:33:54 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:39:29+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:39:31.344967366 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:40:09 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:45:45+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:45:47.648195821 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:46:25 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:51:57+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:51:59.375698405 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:52:37 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:58:08+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:58:10.351831486 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:58:48 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:04:18+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:04:20.508343992 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:04:58 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:10:34+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:10:36.386419709 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:11:14 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:16:49+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:16:51.355276806 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:17:29 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:23:08+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:23:10.378101417 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:23:48 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:29:25+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:29:27.335066670 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:30:05 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:35:46+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:35:48.472699115 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:36:26 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:41:59+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:42:01.443171027 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:42:39 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:48:10+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:48:12.436579371 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:48:50 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:54:29+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:54:31.356538438 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:55:09 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:00:49+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:00:51.559581579 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:01:29 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:07:09+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:07:11.531756889 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:07:49 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:13:21+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:13:23.314062582 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:14:01 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:19:37+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:19:39.314033800 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:20:17 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:25:54+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:25:56.374778136 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:26:34 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:32:17+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:32:19.443870323 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:32:57 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:38:32+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:38:34.309000303 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:39:12 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:44:59+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:45:02.167616601 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:45:40 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:51:15+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:51:17.306725085 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:51:55 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:57:37+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:57:39.372490748 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:58:17 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:03:48+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:03:50.322231826 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:04:28 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:10:01+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:10:03.417601845 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:10:41 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:16:16+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:16:18.282920464 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:16:56 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:22:32+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:22:34.319045002 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:23:12 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:28:43+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:28:46.272559964 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:29:24 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:34:59+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:35:01.401590594 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:35:39 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:41:11+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:41:14.218982209 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:41:52 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:47:27+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:47:30.132345875 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:48:07 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:53:37+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:53:39.376650236 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:54:17 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:59:53+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:59:55.524442404 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:00:34 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:06:09+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:06:11.430804028 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:06:49 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:12:29+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:12:31.401386330 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:13:09 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:18:42+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:18:44.422242744 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:19:22 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:24:56+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:24:58.399407699 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:25:36 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:31:09+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:31:11.335536234 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:31:49 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:37:20+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:37:22.476961042 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:38:00 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:43:43+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:43:45.436316877 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:44:23 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:49:58+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:50:01.156722263 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:50:38 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:56:10+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:56:12.371332627 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:56:50 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T10:02:29+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 10:02:32.229016761 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 10:03:09 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T10:08:44+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 10:08:47.153760499 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 10:09:24 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T10:15:05+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model Qwen/Qwen3-4B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 10:15:07.445636884 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 10:15:45 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
